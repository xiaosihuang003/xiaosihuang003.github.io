---
title: "Machine Learning 3: Lineaarinen luokittelu"
subtitle: "DATA.ML.100 ¬∑ Joni K√§m√§r√§inen ¬∑ ma 1.9.2025 ¬∑ K1704"
date: 2025-09-02
lang: fi
excerpt: "Kertaamme lineaarisen regression ‚Üí yksinkertainen vertailutaso ‚Üí luokittelu. k-NN (et√§isyys/k/monimutkaisuus), suoran sovituksen n√§k√∂kulma, askels√§√§nt√∂, logistinen (sigmoidinen) ulostulo, MSE-gradientit sek√§ miksi ei ole analyyttista ratkaisua ‚Äî pohjustus neuroverkoille."
tags: ["Joni K√§m√§r√§inen","machine-learning","linear-classification","step-function", "sigmoid-function", "logistic"]
draft: false
---
<details>
<summary><strong>Sis√§llysluettelo (avaa ‚ñ∂Ô∏è tarvittaessa)</strong></summary>

- [Osa 1 : Viime viikon kurssin kertaus](#osa-1--viime-viikon-kurssin-kertaus)
  - [1.1 Malli ja opetusaineisto](#11-malli-ja-opetusaineisto)
  - [1.2 Virhe ja miten se ratkaistiin](#12-virhe-ja-miten-se-ratkaistiin)
  - [1.3 Miksi analyyttinen ratkaisu on t√§rke√§](#13-miksi-analyyttinen-ratkaisu-on-t√§rke√§)
  - [1.4 Mink√§lainen ongelma t√§m√§ on](#14-mink√§lainen-ongelma-t√§m√§-on)
- [Osa 2 : Vertailutaso ja luokittelu](#osa-2--vertailutaso-ja-luokittelu)
  - [2.1 Yksinkertainen vertailutaso](#21-yksinkertainen-vertailutaso)
  - [2.2 Siirtyminen luokitteluun](#22-siirtyminen-luokitteluun)
  - [2.3 Ulostulon tyypin muutos](#23-ulostulon-tyypin-muutos)
  - [2.4 Mit√§ havainto on](#24-mit√§-havainto-on)
- [Osa 3 : Esimerkki: Hobitit vs. haltiat](#osa-3--esimerkki-hobitit-vs-haltiat)
  - [3.1 Oppiminen opetusesimerkeist√§](#31-oppiminen-opetusesimerkeist√§)
  - [3.2 √ñrkkien ansatarina](#32-√∂rkkien-ansatarina)
  - [3.3 Datan generointi](#33-datan-generointi)
  - [3.4 P√§√§llekk√§isyyden hyv√§ksyminen](#34-p√§√§llekk√§isyyden-hyv√§ksyminen)
  - [3.5 Siirtyminen kaksiulotteiseen dataan](#35-siirtyminen-kaksiulotteiseen-dataan)
  - [3.6 Koulutus vs. p√§√§ttely](#36-koulutus-vs-p√§√§ttely)
  - [3.7 Ensimm√§inen luokitteluidea](#37-ensimm√§inen-luokitteluidea)
- [Osa 4 : L√§himm√§n naapurin luokitin](#osa-4--l√§himm√§n-naapurin-luokitin)
  - [4.1 Nimi ja idea](#41-nimi-ja-idea)
  - [4.2 Koulutus (talleta kaikki)](#42-koulutus-talleta-kaikki)
  - [4.3 P√§√§ttely (etsi-l√§hin-ja-kopioi-sen-luokka)](#43-p√§√§ttely-etsi-l√§hin-ja-kopioi-sen-luokka)
  - [4.4 Luokittelun vertailutasot](#44-luokittelun-vertailutasot)
  - [4.5 Yhteenveto](#45-yhteenveto)
- [Osa 5 : k-NN huomioita ‚Üí suoran sovitus](#osa-5--k-nn-huomioita--suoran-sovitus)
  - [5.1 Mit√§ voimme s√§√§t√§√§ k-NN:ss√§](#51-mit√§-voimme-s√§√§t√§√§-k-nnss√§)
  - [5.2 Voimmeko luokitella sovittamalla suoran?](#52-voimmeko-luokitella-sovittamalla-suoran)
  - [5.3 Miten mittaamme virheen luokittelussa?](#53-miten-mittaamme-virheen-luokittelussa)
- [Osa 6 : Suoran sovituksesta askel- ja sigmoidifunktioon](#osa-6--suoran-sovituksesta-askel--ja-sigmoidifunktioon)
  - [6.1 Askel-s√§√§nt√∂ ja erotin](#61-askel-s√§√§nt√∂-ja-erotin)
  - [6.2 L√§hent√§minen logistisella (sigmoidilla)](#62-l√§hent√§minen-logistisella-sigmoidilla)
  - [6.3 Opetussignaali (toistaiseksi)](#63-opetussignaali-toistaiseksi)
- [Osa 7 : MSE sigmoid-ulostulolla, gradientit (askel askeleelta, selke√§sti)](#osa-7--mse-sigmoid-ulostulolla-gradientit-askel-askeleelta-selke√§sti)
  - [7.1 Malli, tavoitteet, h√§vi√∂](#71-malli-tavoitteet-h√§vi√∂)
  - [7.2 Miksi ‚àÇz·µ¢/‚àÇb = 1 ja ‚àÇz·µ¢/‚àÇa = x·µ¢?](#72-miksi-‚àÇz·µ¢‚àÇb--1-ja-‚àÇz·µ¢‚àÇa--x·µ¢)
  - [7.3 Sigmoidin derivaatta](#73-sigmoidin-derivaatta)
  - [7.4 Derivoi MSE (ketjus√§√§nt√∂)](#74-derivoi-mse-ketjus√§√§nt√∂)
  - [7.5 Mit√§ kukin tekij√§ tarkoittaa](#75-mit√§-kukin-tekij√§-tarkoittaa)
  - [7.6 Miksi ei ole suljettua ratkaisua](#76-miksi-ei-ole-suljettua-ratkaisua)

</details>

## Osa 1 : Viime viikon kurssin kertaus

![Board notes](/images/docs/Lecture3_Linearclassification/1.png)

L√§hdimme liikkeelle ajatuksesta, ett√§ yksi havainto voidaan kirjoittaa funktiona $y = f(x)$. T√§m√§ tekee siit√§ meille koneoppimisongelman: malli antaa ennusteen $\hat{y}$ ja vertaamme sit√§ todelliseen arvoon $y$.

<br />

### 1.1 Malli ja opetusaineisto

K√§ytimme hyvin yksinkertaista lineaarista mallia,
$$
\hat{y} = a x + b ,
$$
miss√§ $a$ ja $b$ ovat opittavia parametreja. Kun meill√§ on $N$ esimerkki√§, opetusaineisto on
$$
(x_i, y_i), \quad i = 1, \ldots, N .
$$

<br />

### 1.2 Virhe ja miten se ratkaistiin

Valitaksemme $a$:n ja $b$:n minimoimme virheen todellisten arvojen ja ennusteiden v√§lill√§:
$$
\mathcal{L} = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 .
$$
Sitten asetimme gradientit nollaksi saadaksemme optimaalisen ratkaisun:
$$
\frac{\partial \mathcal{L}}{\partial a} = 0, \qquad
\frac{\partial \mathcal{L}}{\partial b} = 0 .
$$
T√§m√§ antaa analyyttisen ratkaisun (samaa k√§ytimme my√∂s kotiteht√§v√§ss√§).

<br />

### 1.3 Miksi analyyttinen ratkaisu on t√§rke√§

Se on eritt√§in nopea laskea: sijoitamme pisteet, k√§yt√§mme summia ja saamme arvion millisekunneissa. Nopeus on j√§rkev√§√§ reaaliaikaisissa tilanteissa (luentoesimerkkin√§ oli lentokonej√§rjestelm√§, joka tekee p√§√§t√∂ksi√§ noin tuhat kertaa sekunnissa).

<br />

### 1.4 Mink√§lainen ongelma t√§m√§ on

Tuloksemme on reaaliarvo, $y \in \mathbb{R}$, joten kyseess√§ on **regressio-ongelma**. Regressio ilmenee jatkuvasti, ja mit√§ tahansa my√∂hemmin teemmekin, vertaamme sit√§ ensin lineaariseen regressioon.

---

## Osa 2 : Vertailutaso ja luokittelu

![Board notes](/images/docs/Lecture3_Linearclassification/2.png)

<br />

### 2.1 Yksinkertainen vertailutaso

Arvioidaksemme, onko lineaarinen regressiomme ‚Äúriitt√§v√§n hyv√§‚Äù, lis√§simme ensin hyvin yksinkertaisen vertailutason: **ei opita mit√§√§n** ‚Äî ennustetaan vain kaikkien opetustavoitteiden keskiarvo. Taululla t√§m√§ kirjoitettiin muodossa ‚Äú$\hat{y}$ = average of all training data $y_i$‚Äù, eli
$$
\hat{y}=\frac{1}{N}\sum_{i=1}^{N} y_i .
$$
T√§m√§n laskeminen on triviaalia, ja t√§llainen baseline kannattaa aina toteuttaa. Lineaarisen regression pit√§isi olla selv√§sti parempi kuin t√§m√§; jos ei ole, niin datassa tai prosessissa on jotain pieless√§.

<br />

### 2.2 Siirtyminen luokitteluun

T√§st√§ siirryimme toiseen t√§rke√§√§n koneoppimisongelmaan: **luokittelu**. T√§m√§n luennon p√§√§viesti on, ett√§ sama lineaarinen sovitus voi toimia my√∂s luokittelussa, mutta tarvitsemme muutamia pieni√§ temppuja.

<br />

### 2.3 Ulostulon tyypin muutos

T√§√§ll√§ ulostulo **ei ole reaaliluku**. Sen sijaan se on jokin **diskreeteist√§ arvoista** (luokista). Taululla:
$$
y \in \{1,2,3\}.
$$
Teht√§v√§n√§ on siis p√§√§tt√§√§, mihin luokkaan sy√∂te kuuluu.

<br />

### 2.4 Mit√§ havainto on

$x$ on havainto. Se voi olla monenlaista:
- kuva (luokan esimerkeiss√§ k√§ytettiin usein kuvia),
- l√§mp√∂tila,
- √§√§ni,
- tai n√§iden yhdistelm√§.

Tyypillinen esimerkki on autonominen ajaminen: j√§rjestelm√§ ottaa kuvia monta kertaa sekunnissa ja sen t√§ytyy tunnistaa jalankulkijat, ajoneuvot ja py√∂r√§ilij√§t ja siirt√§√§ tiedot ajoneuvoj√§rjestelm√§lle. T√§m√§ tekee siit√§ luokitteluongelman, jossa k√§ytet√§√§n diskreettej√§ luokkia.

---

## Osa 3 : Esimerkki: Hobitit vs. haltiat

![Board notes](/images/docs/Lecture3_Linearclassification/3.png)

<br />

### 3.1 Oppiminen opetusesimerkeist√§

Meill√§ on edelleen malli $y = f(x)$, ja funktion $f$ parametrit opitaan opetusesimerkeist√§. T√§m√§ osa toimii samalla tavalla kuin regressio: ker√§√§ dataa ja sovi funktio.  

<br />

### 3.2 √ñrkkien ansatarina

Luento esitteli tarinan. Me olemme √∂rkkej√§ ja meill√§ on ansa. Jos ansa nappaa hobitin, se on hyv√§ juttu, koska sy√∂mme niit√§. Jos se nappaa haltian, se on vaarallista, ja olemme pulassa. Havainto $x$ voisi siis olla olennon **pituus**.  

Oletamme, ett√§ hobitit ovat lyhyempi√§ ja haltiat pidempi√§. T√§t√§ ominaisuutta k√§yt√§mme erottelemaan heid√§t.  

<br />

### 3.3 Datan generointi

Simuloidaksemme oletamme, ett√§ Keski-Maassa pituuden jakauma on normaalijakauma, kuten el√§inten piirteet meid√§n maailmassa. Generoimme viisi satunnaisotosta hobiteille ja viisi haltioille. Hobitit merkit√§√§n yhdell√§ v√§rill√§, haltiat toisella.  

N√§ytteiss√§ huomaamme yhden hyvin pitk√§n hobitin ja yhden melko lyhyen haltian. T√§m√§ tarkoittaa, ett√§ luokat eiv√§t ole t√§ysin erillisi√§, vaan menev√§t p√§√§llekk√§in. Todellisessa el√§m√§ss√§ t√§m√§ on tyypillist√§, ja se tarkoittaa, ett√§ emme voi odottaa 100 % luokittelutarkkuutta.  

<br />

### 3.4 P√§√§llekk√§isyyden hyv√§ksyminen

Koska luokat menev√§t p√§√§llekk√§in, meid√§n t√§ytyy sallia virheit√§. Riippumatta j√§rjestelm√§st√§, joitakin virheit√§ tapahtuu.  
- Esimerkki: sy√∂v√§n havaitsemisessa kaikkia sy√∂pi√§ ei voida havaita, ja joskus terveet n√§ytteet luokitellaan v√§√§rin sy√∂p√§isiksi.  

Siksi luokittelussa on hyv√§ksytt√§v√§ tietty virheaste.  

<br />

### 3.5 Siirtyminen kaksiulotteiseen dataan

T√§h√§n asti olemme k√§ytt√§neet vain yht√§ ominaisuutta: pituutta. Mutta voisimme lis√§t√§ toisen, kuten painon. Oletus: hobitit voivat olla lyhyempi√§ ja painavampia, kun taas haltiat pidempi√§ ja hoikempia.  

Se antaa meille **kaksiulotteisen datan**:  
- $x_1 =$ pituus  
- $x_2 =$ paino  

Nyt jokainen opetuspiste on pari $(x_1, x_2)$, ja $y$ on luokittelutunnus. Opetusaineistossa voimme antaa numeeriset tunnukset, esim. $y=0$ hobitille ja $y=1$ haltiolle.  

<br />

### 3.6 Koulutus vs. p√§√§ttely

Koneoppimisessa erotamme yleens√§ kaksi vaihetta:  
1. **Koulutus** ‚Äî opitaan mallin parametrit merkityist√§ esimerkeist√§.  
2. **P√§√§ttely** ‚Äî testataan mallia uudella n√§ytteell√§ luokan p√§√§tt√§miseksi.  

Jos siis uusi piste tulee sis√§√§n, teht√§v√§ on p√§√§tt√§√§, onko se hobitti vai haltia.  

<br />

### 3.7 Ensimm√§inen luokitteluidea

Luento n√§ytti taululla uuden n√§ytteen. Opiskelijat miettiv√§t, miten se luokiteltaisiin. Intuitiivinen vastaus oli: luokittele hobitiksi, koska se on hyvin l√§hell√§ muita hobittipisteit√§.  

T√§m√§ johtaa luonnollisesti yhteen ensimm√§isist√§ luokittelumenetelmist√§: k√§ytet√§√§n l√§heisyytt√§ opetusesimerkkeihin.

---

## Osa 4 : L√§himm√§n naapurin luokitin

![Board notes](/images/docs/Lecture3_Linearclassification/4.png)

<br />

### 4.1 Nimi ja idea
Siirryimme ‚Äúl√§heisyys‚Äù-intuition kautta konkreettiseen menetelm√§√§n: **l√§himm√§n naapurin luokitin** (1-NN ja my√∂hemmin $k$-NN). Idea on yksinkertainen: uuden n√§ytteen tulisi saada sama luokka kuin l√§himm√§n opetusesimerkin.

<br />

### 4.2 Koulutus (talleta kaikki)
Koulutus on t√§ss√§ triviaalia: **talletamme vain kaikki opetusesimerkit** (piirteet ja niiden luokkatunnukset). T√§ss√§ vaiheessa ei soviteta parametreja.

<br />

### 4.3 P√§√§ttely (etsi l√§hin ja kopioi sen luokka)
Kun uusi n√§yte saapuu, teemme seuraavat laskut:  
1. Laske et√§isyys uuden n√§ytteen ja **kaikkien** opetusesimerkkien v√§lill√§.  
   (Mik√§ tahansa j√§rkev√§ et√§isyys k√§y; tarvitsemme vain tavan verrata, kuka on l√§hin.)  
2. Jos et√§isyys on **pienempi kuin paras t√§h√§n menness√§**, p√§ivit√§ paras ja **valitse** kyseinen esimerkki.  
3. **Palauta luokkatunnus** silt√§ esimerkilt√§, jolla on **lyhin et√§isyys**.  

Siin√§ koko algoritmi. Toteutamme t√§m√§n Pythonilla harjoituksissa.

<br />

### 4.4 Luokittelun vertailutasot
Asetimme my√∂s vertailutasot arvioidaksemme, onko luokitin hyv√§:  
- **Satunnainen luokka** ‚Äî tulosta satunnainen tunnus luokkasetist√§.  
- **Yleisin luokka** ‚Äî tulosta aina se tunnus, joka esiintyy useimmin opetusaineistossa.  

Toinen baseline on yleens√§ parempi kuin satunnainen, erityisesti kun yksi luokka **hallitsee** (esim. 99 % n√§ytteist√§ on yht√§ luokkaa). Jos ‚Äúyleisin luokka‚Äù antaa jo 99 % tarkkuuden, menetelm√§mme t√§ytyy ylitt√§√§ **sen** luvun ollakseen merkityksellinen.

<br />

### 4.5 Yhteenveto
- L√§himm√§n naapurin algoritmi on **hyvin yksinkertainen**, mutta **yll√§tt√§v√§n tehokas**.  
- Vertaa aina **yksinkertaiseen vertailutasoon** datallesi; se kertoo, kuinka vaikea ongelma on ja mink√§ suorituskyvyn meid√§n t√§ytyy ylitt√§√§.

---
## Osa 5 : k-NN huomioita ‚Üí suoran sovitus

![Board notes](/images/docs/Lecture3_Linearclassification/5.png)

<br />

### 5.1 Mit√§ voimme s√§√§t√§√§ k-NN:ss√§
Kun alamme tutkia, mit√§ voimme muuttaa l√§himm√§n naapurin menetelm√§ss√§, avautuu kokonaan uusi maailma:

- **Et√§isyys.** Voimme valita et√§isyysmitan.  
  1D:ss√§ euklidinen on  
  $$
  d(x,x_i)=\sqrt{(x-x_i)^2},
  $$
  ja **kaupunkilohko (L1)** -et√§isyys on  
  $$
  d(x,x_i)=|x-x_i|.
  $$
  On monia muitakin et√§isyyksi√§; valitaan se, joka sopii datalle.

- **Naapurien m√§√§r√§.** $k$ voi olla $1,3,\dots$  
  Kahdessa luokassa $k=2$ voi tasoittua (yksi hobitti, yksi haltia), joten $k=3$ v√§ltt√§√§ tasapelin.

- **Laskenta-aika.** Perus 1-NN k√§y l√§pi kaikki opetuspisteet, ja suurilla dataseteilla se voi olla hidasta. On kuitenkin olemassa **nopeita NN-menetelmi√§** (usein likim√§√§r√§isi√§), jotka jakavat avaruuden ja nopeuttavat hakua.

- **1D vs. 2D et√§isyydet k-NN:ss√§:**  
  **1D:ss√§** euklidinen (L2) ja kaupunkilohko (L1) yhtyv√§t:  
  $$
  |x-x_i|.
  $$  
  **2D:ss√§ ja korkeammissa ulottuvuuksissa** ne eroavat:  
  - 2D euklidinen (L2):  
    $$
    d(\mathbf{x},\mathbf{x_i})=\sqrt{(x_1-x_{i1})^2+(x_2-x_{i2})^2},
    $$
    joka tuottaa ympyr√§nmuotoisia naapuruuksia.  
  - 2D kaupunkilohko (L1):  
    $$
    d(\mathbf{x},\mathbf{x_i})=|x_1-x_{i1}|+|x_2-x_{i2}|,
    $$
    joka tuottaa timantinmuotoisia naapuruuksia.  

  T√§m√§ ero muuttaa **k-NN:n p√§√§t√∂srajat**: L2 antaa pehme√§mm√§t, py√∂re√§mm√§t rajat; L1 seuraa koordinaattiakseleita. Oikea valinta riippuu datan geometriasta.

**ü§î Kotiteht√§v√§:** Anna 2D-pisteet, joissa **1-NN** ja **3-NN** antavat **eri** luokitukset.

<br />

### 5.2 Voimmeko luokitella sovittamalla suoran?
![Board notes](/images/docs/Lecture3_Linearclassification/6.png)

Kokeillaan esitt√§√§ luokittelu **suoran sovituksena**. Annamme luokkatavoitteet
$$
y=-1 \;\text{hobitille}, \qquad y=+1 \;\text{haltialle}.
$$
Sitten sovitamme suoran
$$
\hat y = a x + b
$$
n√§ille $(x,y)$-pareille (ihan kuten regressiossa).  
Kun suora on sovitettu, k√§yt√§mme yksinkertaista **luokittelus√§√§nt√∂√§**:
$$
\hat y < 0 \Rightarrow \text{hobitti}, \qquad
\hat y > 0 \Rightarrow \text{haltia}.
$$
Piste, jossa $\hat y=0$, on **erotin** (p√§√§t√∂sraja).

Luokan esimerkiss√§ sovitimme suoran ja testasimme muutamia uusia n√§ytteit√§; t√§ll√§ datalla yll√§ oleva s√§√§nt√∂ luokitteli ne kaikki oikein.

![Board notes](/images/docs/Lecture3_Linearclassification/7.png)

<br />

### 5.3 Miten mittaamme virheen luokittelussa?
Luokittelussa virhe on **bin√§√§rinen**:  
- jos $\hat y$ antaa **oikean** luokan ‚áí $\text{err}=0$,  
- jos $\hat y$ antaa **v√§√§r√§n** luokan ‚áí $\text{err}=1$.  

‚ÄúEhk√§‚Äù-tuloksia ei sallita.  
Huomaa ristiriita: suoran sovitus minimoi **neli√∂virheen**, ei t√§t√§ 0/1-virhett√§. T√§m√§ ristiriita voi aiheuttaa ongelmia my√∂hemmin ‚Äî korjaamme sen seuraavaksi.

---
## Osa 6 : Suoran sovituksesta askel- ja sigmoidifunktioon

![Board notes](/images/docs/Lecture3_Linearclassification/8.png)

<br />

### 6.1 Askel-s√§√§nt√∂ ja erotin

Pid√§mme sovitetun suoran $\hat y = a x + b$ ja m√§√§rittelemme **erottimen** kohdassa $\hat y=0$.  
Taululla askels√§√§nt√∂ k√§ytti **$\pm1$** -luokkia:

$$
y =
\begin{cases}
+1, & x > x_d,\\[4pt]
-1, & x < x_d .
\end{cases}
$$

Sama suoran etumerkill√§:

$$
\hat y < 0 \Rightarrow \text{hobitti}, \qquad
\hat y > 0 \Rightarrow \text{haltia}.
$$

![Board notes](/images/docs/Lecture3_Linearclassification/9.png)

<br />

### 6.2 L√§hent√§minen logistisella (sigmoidilla)

Koska kova askel on ep√§jatkuva, k√§yt√§mme sen pehme√§√§ approksimaatiota, logistista (‚Äúlogsig‚Äù):

$$
\operatorname{logsig}(x)=\frac{1}{1+e^{-x}} .
$$

Yhdistet√§√§n se suoran kanssa:

$$
\hat y \;=\; \operatorname{logsig}(a x + b)
=\frac{1}{1+e^{-(a x + b)}} .
$$

T√§ss√§ $a$ hallitsee, kuinka jyrkk√§ siirtym√§ on, ja $b$ siirt√§√§ kynnyst√§.  
Luokat nimet√§√§n uudelleen $[0,1]$ -alueelle (hobitti $=0$, haltia $=1$).

<br />

### 6.3 Opetussignaali (toistaiseksi)

T√§ll√§ pehme√§ll√§ ulostulolla voimme yh√§ minimoida **keskineli√∂virheen** tavoitteiden $\{0,1\}$ ja $\hat y$:n v√§lill√§ sovittaaksemme $a$:n ja $b$:n.


---
## Osa 7 : MSE sigmoid-ulostulolla, gradientit (askel askeleelta, selke√§sti)

![Board notes](/images/docs/Lecture3_Linearclassification/10.png)

<br />

### 7.1 Malli, tavoitteet, h√§vi√∂

Malli (1D-piirre \(x\)):

$$
z_i = a x_i + b
$$

$$
\hat y_i = \sigma(z_i) = \frac{1}{1+e^{-z_i}}
$$

Tavoitteet:

$$
y_i \in \{0,1\}
$$

Keskineli√∂virhe (MSE):

$$
\mathcal{L}_{\text{MSE}}=\frac{1}{N}\sum_{i=1}^{N}\bigl(y_i-\hat y_i\bigr)^2
$$

<br />

### 7.2 Miksi $ \frac{\partial z_i}{\partial b} = 1 $ ja $ \frac{\partial z_i}{\partial a} = x_i $?

Suora:

$$
z_i = a x_i + b
$$

Derivointi \(b\):n suhteen:

$$
\frac{\partial z_i}{\partial b} = 1
$$

Derivointi \(a\):n suhteen:

$$
\frac{\partial z_i}{\partial a} = x_i
$$

Eli \(b\):n muutos siirt√§√§ suoraa yl√∂s/alas, \(a\):n muutos kallistaa sit√§ mittakaavalla \(x_i\).

<br />

### 7.3 Sigmoidin derivaatta

M√§√§ritelm√§:

$$
\sigma(z)=\frac{1}{1+e^{-z}}
$$

Derivaatta:

$$
\frac{d\sigma}{dz}=\sigma(z)\bigl(1-\sigma(z)\bigr)
$$

<br />

### 7.4 Derivoi MSE (ketjus√§√§nt√∂)

J√§√§nn√∂s:

$$
e_i = y_i - \hat y_i
$$

H√§vi√∂ j√§√§nn√∂ksen√§:

$$
\mathcal{L}_{\text{MSE}}=\frac{1}{N}\sum_{i=1}^{N} e_i^{\,2}
$$

Ulkokerroksen neli√∂n derivaatta:

$$
\frac{\partial}{\partial e_i}\bigl(e_i^{\,2}\bigr)=2e_i
$$

J√§√§nn√∂s vs. ulostulo:

$$
\frac{\partial e_i}{\partial \hat y_i} = -1
$$

Sigmoidiketju:

$$
\frac{\partial \hat y_i}{\partial a} = \sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\cdot x_i
$$

$$
\frac{\partial \hat y_i}{\partial b} = \sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\cdot 1
$$

Koko ketju:  

$$
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial a}
=\frac{2}{N}\sum_{i=1}^{N}\bigl(\hat y_i-y_i\bigr)\,\sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\,x_i
$$

$$
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial b}
=\frac{2}{N}\sum_{i=1}^{N}\bigl(\hat y_i-y_i\bigr)\,\sigma(z_i)\bigl(1-\sigma(z_i)\bigr)
$$

<br />

### 7.5 Mit√§ kukin tekij√§ tarkoittaa

- J√§√§nn√∂s:
$$
\hat y_i - y_i
$$

- Sigmoidin jyrkkyys:
$$
\sigma(z_i)\bigl(1-\sigma(z_i)\bigr)
$$

- Sy√∂temittakaava (vain \(a\):lle):
$$
x_i
$$

Sigmoidin keskell√§ (0.5) jyrkkyys on suuri ‚Üí vahva oppimissignaali. L√§hell√§ 0 tai 1 se on pieni ‚Üí saturaatiota.

<br />

### 7.6 Miksi ei ole suljettua ratkaisua

Parametrit \(a,b\) ovat **sigmoidin sis√§ll√§** summassa. Nollaan asettaminen ei yksinkertaistu suljetuksi muodoksi (toisin kuin lineaarisessa regressiossa).  
Siksi meid√§n t√§ytyy **iteroida**:

Oppimisnopeusp√§ivitys:

$$
a \leftarrow a - \eta\,\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial a}
$$

$$
b \leftarrow b - \eta\,\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial b}
$$

Toista kunnes h√§vi√∂ ei en√§√§ pienene.
