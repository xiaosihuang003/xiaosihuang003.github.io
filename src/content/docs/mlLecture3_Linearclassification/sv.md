---
title: "Maskininl√§rning 3: Linj√§r klassificering"
subtitle: "DATA.ML.100 ¬∑ Joni K√§m√§r√§inen ¬∑ m√•n 1.9.2025 ¬∑ K1704"
date: 2025-09-02
lang: sv
excerpt: "Vi repeterar linj√§r regression ‚Üí enkel baslinje ‚Üí klassificering. k-NN (avst√•nd/k/komplexitet), perspektivet av linj√§r anpassning, stegregel, logistisk (sigmoid) utg√•ng, MSE-gradienter och varf√∂r det inte finns en sluten formell l√∂sning ‚Äî f√∂rberedelse f√∂r neurala n√§tverk."
tags: ["Joni K√§m√§r√§inen","machine-learning","linear-classification","step-function", "sigmoid-function", "logistic"]
draft: false
---
<details>
<summary><strong>Inneh√•llsf√∂rteckning (klicka ‚ñ∂Ô∏è vid behov)</strong></summary>

- [Del 1 : Repetition fr√•n f√∂rra veckans kurs](#del-1--repetition-fr√•n-f√∂rra-veckans-kurs)
  - [1.1 Modell och tr√§ningsdata](#11-modell-och-tr√§ningsdata)
  - [1.2 Fel och hur vi l√∂ste det](#12-fel-och-hur-vi-l√∂ste-det)
  - [1.3 Varf√∂r den analytiska l√∂sningen √§r viktig](#13-varf√∂r-den-analytiska-l√∂sningen-√§r-viktig)
  - [1.4 Vilken typ av problem detta √§r](#14-vilken-typ-av-problem-detta-√§r)
- [Del 2 : Baslinje och klassificering](#del-2--baslinje-och-klassificering)
  - [2.1 En enkel baslinje](#21-en-enkel-baslinje)
  - [2.2 √ñverg√•ng till klassificering](#22-√∂verg√•ng-till-klassificering)
  - [2.3 Utdata √§ndras](#23-utdata-√§ndras)
  - [2.4 Vad observationen √§r](#24-vad-observationen-√§r)
- [Del 3 : Exempel: Hobbits vs. alver](#del-3--exempel-hobbits-vs-alver)
  - [3.1 Att l√§ra fr√•n tr√§ningsdata](#31-att-l√§ra-fr√•n-tr√§ningsdata)
  - [3.2 Orchernas f√§lla](#32-orchernas-f√§lla)
  - [3.3 Generera data](#33-generera-data)
  - [3.4 Acceptera √∂verlappning](#34-acceptera-√∂verlappning)
  - [3.5 Tv√•dimensionell data](#35-tv√•dimensionell-data)
  - [3.6 Tr√§ning vs. inferens](#36-tr√§ning-vs-inferens)
  - [3.7 F√∂rsta klassificeringsid√©n](#37-f√∂rsta-klassificeringsid√©n)
- [Del 4 : N√§rmaste granne-klassificerare](#del-4--n√§rmaste-granne-klassificerare)
  - [4.1 Namn och id√©](#41-namn-och-id√©)
  - [4.2 Tr√§ning (spara allt)](#42-tr√§ning-spara-allt)
  - [4.3 Inferens (hitta den n√§rmaste och kopiera dess etikett)](#43-inferens-hitta-den-n√§rmaste-och-kopiera-dess-etikett)
  - [4.4 Enkla baslinjer f√∂r klassificering](#44-enkla-baslinjer-f√∂r-klassificering)
  - [4.5 Slutsatser](#45-slutsatser)
- [Del 5 : k-NN √∂verv√§ganden ‚Üí linjeanpassning](#del-5--k-nn-√∂verv√§ganden--linjeanpassning)
  - [5.1 Vad vi kan justera i k-NN](#51-vad-vi-kan-justera-i-k-nn)
  - [5.2 Kan vi klassificera genom att anpassa en linje?](#52-kan-vi-klassificera-genom-att-anpassa-en-linje)
  - [5.3 Hur m√§ter vi fel i klassificering?](#53-hur-m√§ter-vi-fel-i-klassificering)
- [Del 6 : Fr√•n linjeanpassning till steg, sedan till sigmoid](#del-6--fr√•n-linjeanpassning-till-steg-sedan-till-sigmoid)
  - [6.1 Stegregel och diskriminator](#61-stegregel-och-diskriminator)
  - [6.2 Approximation av steg med logistisk (sigmoid)](#62-approximation-av-steg-med-logistisk-sigmoid)
  - [6.3 Tr√§ningssignal (f√∂r nu)](#63-tr√§ningssignal-f√∂r-nu)
- [Del 7 : MSE med sigmoidutg√•ng, gradienter (steg-f√∂r-steg, tydligt)](#del-7--mse-med-sigmoidutg√•ng-gradienter-steg-f√∂r-steg-tydligt)
  - [7.1 Modell, m√•l, f√∂rlust](#71-modell-m√•l-f√∂rlust)
  - [7.2 Varf√∂r ‚àÇz·µ¢/‚àÇb = 1 och ‚àÇz·µ¢/‚àÇa = x·µ¢?](#72-varf√∂r-‚àÇz·µ¢‚àÇb--1-och-‚àÇz·µ¢‚àÇa--x·µ¢)
  - [7.3 Derivatan av sigmoid](#73-derivatan-av-sigmoid)
  - [7.4 Derivera MSE (kedjeregel)](#74-derivera-mse-kedjeregel)
  - [7.5 Vad varje faktor betyder](#75-vad-varje-faktor-betyder)
  - [7.6 Varf√∂r vi inte har en sluten l√∂sning](#76-varf√∂r-vi-inte-har-en-sluten-l√∂sning)

</details>

## Del 1 : Repetition fr√•n f√∂rra veckans kurs

![Board notes](/images/docs/Lecture3_Linearclassification/1.png)

Vi b√∂rjade med id√©n att en observation kan skrivas som en funktion $y = f(x)$. Detta g√∂r det till ett maskininl√§rningsproblem: modellen ger en prediktion $\hat{y}$ och vi j√§mf√∂r den med det sanna v√§rdet $y$.

<br />

### 1.1 Modell och tr√§ningsdata

Vi anv√§nde en mycket enkel linj√§r modell,
$$
\hat{y} = a x + b ,
$$
d√§r $a$ och $b$ √§r parametrar som ska l√§ras. Med $N$ exempel √§r tr√§ningsdatan
$$
(x_i, y_i), \quad i = 1, \ldots, N .
$$

<br />

### 1.2 Fel och hur vi l√∂ste det

F√∂r att v√§lja $a$ och $b$ minimerade vi felet mellan de sanna v√§rdena och v√•ra prediktioner:
$$
\mathcal{L} = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 .
$$
Sedan satte vi gradienterna lika med noll f√∂r att f√• det optimala:
$$
\frac{\partial \mathcal{L}}{\partial a} = 0, \qquad
\frac{\partial \mathcal{L}}{\partial b} = 0 .
$$
Detta ger en analytisk l√∂sning (samma som vi anv√§nde i heml√§xan).

<br />

### 1.3 Varf√∂r den analytiska l√∂sningen √§r viktig

Den √§r mycket snabb att ber√§kna: vi stoppar in punkterna, anv√§nder summorna och f√•r resultatet p√• millisekunder. Denna hastighet √§r avg√∂rande i realtidssituationer (exemplet i f√∂rel√§sningen var ett flygplanssystem som fattar rotationsbeslut ungef√§r tusen g√•nger per sekund).

<br />

### 1.4 Vilken typ av problem detta √§r

V√•r output √§r ett reellt v√§rde, $y \in \mathbb{R}$, s√• detta √§r ett **regressionsproblem**. Regression f√∂rekommer hela tiden, och vad vi √§n g√∂r senare j√§mf√∂r vi det f√∂rst med linj√§r regression.

---
## Del 2 : Baslinje och klassificering

![Board notes](/images/docs/Lecture3_Linearclassification/2.png)

<br />

### 2.1 En enkel baslinje

F√∂r att avg√∂ra om v√•r linj√§ra regression √§r ‚Äútillr√§ckligt bra‚Äù lade vi f√∂rst till en mycket enkel baslinje: **l√§r dig ingenting** ‚Äî f√∂ruts√§g helt enkelt medelv√§rdet av alla tr√§ningsm√•l. P√• tavlan skrevs detta som ‚Äú$\hat{y}$ = average of all training data $y_i$‚Äù, dvs.
$$
\hat{y}=\frac{1}{N}\sum_{i=1}^{N} y_i .
$$
Detta √§r trivialt att ber√§kna, och man b√∂r alltid implementera en s√•dan baslinje. V√•r linj√§ra regression b√∂r vara tydligt b√§ttre √§n detta; om inte, √§r det n√•got konstigt med datan och vi beh√∂ver kontrollera den.

<br />

### 2.2 √ñverg√•ng till klassificering

D√§refter gick vi √∂ver till ett annat viktigt ML-problem: **klassificering**. Huvudbudskapet i denna f√∂rel√§sning √§r att samma linj√§ra linjeanpassning ocks√• kan fungera f√∂r klassificering, men vi beh√∂ver n√•gra sm√• trick.

<br />

### 2.3 Utdata √§ndras

H√§r √§r utdata **inte ett reellt v√§rde**. Ist√§llet √§r det ett av **diskreta v√§rden** (etiketter). P√• tavlan:
$$
y \in \{1,2,3\}.
$$
Uppgiften √§r allts√• att best√§mma vilken klass indata tillh√∂r.

<br />

### 2.4 Vad observationen √§r

$x$ √§r observationen. Det kan vara m√•nga saker:
- bild (exemplen i klassen anv√§nde ofta bilder),
- temperatur,
- ljud,
- eller en kombination av dessa.

Ett typiskt exempel √§r sj√§lvk√∂rande bilar: systemet tar bilder m√•nga g√•nger per sekund och m√•ste uppt√§cka fotg√§ngare, fordon och cyklister, och sedan vidarebefordra informationen till k√∂rsystemet. Detta blir ett klassificeringsproblem med diskreta etiketter.

---

## Del 3 : Exempel: Hobbits vs. alver

![Board notes](/images/docs/Lecture3_Linearclassification/3.png)

<br />

### 3.1 Att l√§ra fr√•n tr√§ningsdata

Vi har fortfarande modellen $y = f(x)$, och parametrarna f√∂r $f$ l√§rs fr√•n tr√§ningsdata. Denna del fungerar p√• samma s√§tt som regression: samla data och anpassa funktionen.  

<br />

### 3.2 Orchernas f√§lla

F√∂rel√§sningen introducerade en ber√§ttelse. Vi √§r orcher och vi har en f√§lla. Om f√§llan f√•ngar hobbits √§r det bra, vi √§ter dem. Om f√§llan f√•ngar alver √§r det farligt, vi f√•r problem. Observationen $x$ kan allts√• vara varelsens **l√§ngd**.  

Vi antar att hobbits √§r kortare och alver √§r l√§ngre. Detta √§r den egenskap vi anv√§nder f√∂r att skilja dem √•t.  

<br />

### 3.3 Generera data

F√∂r att simulera antar vi att l√§ngdf√∂rdelningen i Midg√•rd √§r normalf√∂rdelad, precis som djuregenskaper i v√•r v√§rld. Vi genererar fem slumpm√§ssiga prover f√∂r hobbits och fem f√∂r alver. Hobbits visas i en f√§rg, alver i en annan.  

I urvalet ser vi en mycket l√•ng hobbit och en ganska kort alv. Detta betyder att klasserna inte √§r helt separata, de √∂verlappar. I verkligheten √§r detta typiskt, och det betyder att vi inte kan f√∂rv√§nta oss 100 % klassificeringsnoggrannhet.  

<br />

### 3.4 Acceptera √∂verlappning

P√• grund av √∂verlappning m√•ste vi acceptera fel. Oavsett vilket system vi bygger kommer n√•gra misstag att intr√§ffa.  
- Exempel: vid cancerdetektion kan inte alla cancerfall uppt√§ckas, och ibland klassificeras friska prover felaktigt som cancer.  

Allts√• m√•ste vi r√§kna med en viss felfrekvens i klassificering.  

<br />

### 3.5 Tv√•dimensionell data

Hittills har vi bara anv√§nt en egenskap: l√§ngd. Men vi kan l√§gga till en annan, som vikt. Antagandet: hobbits kan vara kortare och tyngre, medan alver √§r l√§ngre och smalare.  

Detta ger oss **tv√•dimensionell data**:  
- $x_1 =$ l√§ngd  
- $x_2 =$ vikt  

Nu √§r varje tr√§ningspunkt ett par $(x_1, x_2)$, och $y$ √§r klassetiketten. F√∂r tr√§ningsdatan kan vi tilldela numeriska etiketter, t.ex. $y=0$ f√∂r hobbit och $y=1$ f√∂r alv.  

<br />

### 3.6 Tr√§ning vs. inferens

Inom maskininl√§rning skiljer vi ofta tv√• steg:  
1. **Tr√§ning** ‚Äî vi l√§r modellens parametrar fr√•n m√§rkta exempel.  
2. **Inferens** ‚Äî vi testar modellen p√• ett nytt prov f√∂r att avg√∂ra dess klass.  

S√• om en ny punkt kommer in √§r uppgiften att avg√∂ra om det √§r en hobbit eller en alv.  

<br />

### 3.7 F√∂rsta klassificeringsid√©n

I f√∂rel√§sningen visades ett nytt prov p√• tavlan. Studenterna funderade p√• hur man skulle klassificera det. Den intuitiva l√∂sningen var: klassificera det som hobbit, eftersom det ligger mycket n√§ra andra hobbitpunkter.  

Detta leder naturligt till en av de f√∂rsta klassificeringsmetoderna: att anv√§nda n√§rhet till tr√§ningsprover.

---

## Del 4 : N√§rmaste granne-klassificerare

![Board notes](/images/docs/Lecture3_Linearclassification/4.png)

<br />

### 4.1 Namn och id√©
Vi gick fr√•n ‚Äún√§rhets‚Äù-intuitionen till en konkret metod: **n√§rmsta granne-klassificeraren** (1-NN och senare $k$-NN). Id√©n √§r enkel: ett nytt prov ska f√• samma klass som det n√§rmaste tr√§ningsprovet.

<br />

### 4.2 Tr√§ning (spara allt)
Tr√§ningen √§r trivial h√§r: **vi sparar bara alla tr√§ningsprover** (egenskaper och deras klassetiketter). Inga parametrar anpassas i detta steg.

<br />

### 4.3 Inferens (hitta den n√§rmaste och kopiera dess etikett)
N√§r ett nytt prov kommer g√∂r vi f√∂ljande ber√§kningar:  
1. F√∂r **alla** tr√§ningsprover, ber√§kna avst√•ndet till det nya provet.  
   (Vilket rimligt avst√•nd som helst g√•r bra; vi beh√∂ver bara kunna j√§mf√∂ra vem som √§r n√§rmare.)  
2. Om avst√•ndet √§r **mindre √§n det b√§sta hittills**, uppdatera det b√§sta och **v√§lj** det provet.  
3. **Returnera klassetiketten** f√∂r det prov som har **kortast avst√•nd**.  

Detta √§r hela algoritmen. Vi implementerar den i Python i √∂vningarna.

<br />

### 4.4 Enkla baslinjer f√∂r klassificering
Vi satte ocks√• baslinjer f√∂r att avg√∂ra om v√•r klassificerare √§r bra:  
- **Slumpklass** ‚Äî returnera en slumpm√§ssig etikett fr√•n klassm√§ngden.  
- **Vanligaste etiketten** ‚Äî returnera alltid den etikett som f√∂rekommer mest i tr√§ningsdatan.  

Den andra baslinjen √§r vanligtvis b√§ttre √§n slump, s√§rskilt n√§r en klass **dominerar** (t.ex. 99 % av proverna √§r fr√•n en klass). Om ‚Äúvanligaste etiketten‚Äù redan ger 99 % noggrannhet m√•ste v√•r metod sl√• **det** talet f√∂r att vara meningsfull.

<br />

### 4.5 Slutsatser
- N√§rmaste granne √§r en **mycket enkel** algoritm, men √§nd√• **f√∂rv√•nansv√§rt kraftfull**.  
- J√§mf√∂r alltid mot **enkel baslinje** f√∂r din data; det visar hur sv√•rt problemet √§r och vilken prestanda vi m√•ste sl√•.

---
## Del 5 : k-NN √∂verv√§ganden ‚Üí linjeanpassning

![Board notes](/images/docs/Lecture3_Linearclassification/5.png)

<br />

### 5.1 Vad vi kan justera i k-NN
N√§r vi b√∂rjar studera vad vi kan √§ndra i n√§rmaste granne, √∂ppnar sig en helt ny v√§rld:

- **Avst√•nd.** Vi kan v√§lja avst√•ndsm√•tt.  
  Euklidiskt i 1D √§r  
  $$
  d(x,x_i)=\sqrt{(x-x_i)^2},
  $$
  och ett **city-block (L1)**-avst√•nd √§r  
  $$
  d(x,x_i)=|x-x_i|.
  $$
  Det finns m√•nga andra avst√•nd; vi b√∂r v√§lja det som passar datan.

- **Antal grannar.** $k$ kan vara $1,3,\dots$  
  F√∂r tv√• klasser kan $k=2$ bli oavgjort (en hobbit, en alv), s√• $k=3$ undviker detta.

- **Ber√§kningstid.** En grundl√§ggande 1-NN inneb√§r en loop √∂ver alla tr√§ningspunkter och kan vara l√•ngsam f√∂r enorma dataset, men det finns **snabba NN-metoder** (ofta approximativa) som delar upp rummet och snabbar upp s√∂kningen.

- **1D vs 2D-avst√•nd i k-NN:**  
  I **1D** sammanfaller Euklidiskt (L2) och City-block (L1):  
  $$
  |x-x_i|.
  $$  
  I **2D och h√∂gre dimensioner** skiljer de sig:  
  - Euklidiskt (L2) i 2D:  
    $$
    d(\mathbf{x},\mathbf{x_i})=\sqrt{(x_1-x_{i1})^2+(x_2-x_{i2})^2},
    $$
    vilket ger cirkul√§ra grannskap.  
  - City-block (L1) i 2D:  
    $$
    d(\mathbf{x},\mathbf{x_i})=|x_1-x_{i1}|+|x_2-x_{i2}|,
    $$
    vilket ger diamantformade grannskap.  

  Denna skillnad √§ndrar **k-NN:s beslutsgr√§nser**: med L2 blir de j√§mnare och rundare; med L1 f√∂ljer de koordinataxlarna. Valet beror p√• datans geometri.

**ü§î Heml√§xa:** Ge 2D-punkter d√§r **1-NN** och **3-NN** ger **olika** klassificeringar.

<br />

### 5.2 Kan vi klassificera genom att anpassa en linje?
![Board notes](/images/docs/Lecture3_Linearclassification/6.png)

Vi f√∂rs√∂ker uttrycka klassificering som **linjeanpassning**. Vi tilldelar klasstargets
$$
y=-1 \;\text{f√∂r hobbit}, \qquad y=+1 \;\text{f√∂r alv}.
$$
Sedan anpassar vi en linje
$$
\hat y = a x + b
$$
till dessa $(x,y)$-par (precis som i regression).  
N√§r linjen √§r anpassad anv√§nder vi den enkla **klassificeringsregeln**:
$$
\hat y < 0 \Rightarrow \text{hobbit}, \qquad
\hat y > 0 \Rightarrow \text{alv}.
$$
Punkten d√§r $\hat y=0$ √§r **diskriminatorn** (beslutsgr√§nsen).

I √∂vningen p√• f√∂rel√§sningen anpassade vi linjen och utv√§rderade n√•gra nya prover; med denna data klassificerade regeln dem alla korrekt.

![Board notes](/images/docs/Lecture3_Linearclassification/7.png)

<br />

### 5.3 Hur m√§ter vi fel i klassificering?
F√∂r klassificering √§r felet **bin√§rt**:  
- om $\hat y$ ger **r√§tt** klass ‚áí $\text{err}=0$,  
- om $\hat y$ ger **fel** klass ‚áí $\text{err}=1$.  

Vi till√•ter inga ‚Äúkanske‚Äù-utdata.  
Observera skillnaden: linjeanpassning minimerar **kvadratfel**, inte detta 0/1-fel. Den skillnaden kan orsaka problem senare ‚Äî vi fixar det i n√§sta steg.

---
## Del 6 : Fr√•n linjeanpassning till steg, sedan till sigmoid

![Board notes](/images/docs/Lecture3_Linearclassification/8.png)

<br />

### 6.1 Stegregel och diskriminator

Vi beh√•ller den anpassade linjen $\hat y = a x + b$ och definierar **diskriminatorn** vid $\hat y=0$.  
Stegregeln p√• tavlan anv√§nder **$\pm1$** etiketter:

$$
y =
\begin{cases}
+1, & x > x_d,\\[4pt]
-1, & x < x_d .
\end{cases}
$$

Ekvivalent med linjens tecken:

$$
\hat y < 0 \Rightarrow \text{hobbit}, \qquad
\hat y > 0 \Rightarrow \text{alv}.
$$

![Board notes](/images/docs/Lecture3_Linearclassification/9.png)

<br />

### 6.2 Approximation av steg med logistisk (sigmoid)

Eftersom ett h√•rt steg √§r diskontinuerligt anv√§nder vi dess mjuka approximation, den logistiska (‚Äúlogsig‚Äù):

$$
\operatorname{logsig}(x)=\frac{1}{1+e^{-x}} .
$$

Kombinera den med linjen:

$$
\hat y \;=\; \operatorname{logsig}(a x + b)
=\frac{1}{1+e^{-(a x + b)}} .
$$

H√§r styr $a$ hur skarp √∂verg√•ngen √§r, och $b$ flyttar tr√∂skeln.  
Vi m√§rker om klasserna till intervallet $[0,1]$ (hobbit $=0$, alv $=1$).

<br />

### 6.3 Tr√§ningssignal (f√∂r nu)

Med denna mjuka utg√•ng kan vi fortfarande minimera **medelkvadratfelet** mellan m√•len i $\{0,1\}$ och $\hat y$ f√∂r att anpassa $a$ och $b$.

---

## Del 7 : MSE med sigmoidutg√•ng, gradienter (steg-f√∂r-steg, tydligt)

![Board notes](/images/docs/Lecture3_Linearclassification/10.png)

<br />

### 7.1 Modell, m√•l, f√∂rlust

Modell (1D-funktion \(x\)):

$$
z_i = a x_i + b
$$

$$
\hat y_i = \sigma(z_i) = \frac{1}{1+e^{-z_i}}
$$

M√•l:

$$
y_i \in \{0,1\}
$$

Medelkvadratfel (MSE):

$$
\mathcal{L}_{\text{MSE}}=\frac{1}{N}\sum_{i=1}^{N}\bigl(y_i-\hat y_i\bigr)^2
$$

<br />

### 7.2 Varf√∂r $ \frac{\partial z_i}{\partial b} = 1 $ och $ \frac{\partial z_i}{\partial a} = x_i $?

Vi har en rak linje:

$$
z_i = a x_i + b
$$

Derivera m.a.p. \(b\):

$$
\frac{\partial z_i}{\partial b} = 1
$$

Derivera m.a.p. \(a\):

$$
\frac{\partial z_i}{\partial a} = x_i
$$

Allts√•, √§ndring av \(b\) flyttar linjen upp/ner, √§ndring av \(a\) lutar den beroende av \(x_i\).

<br />

### 7.3 Derivatan av sigmoid

Definition:

$$
\sigma(z)=\frac{1}{1+e^{-z}}
$$

Derivata:

$$
\frac{d\sigma}{dz}=\sigma(z)\bigl(1-\sigma(z)\bigr)
$$

<br />

### 7.4 Derivera MSE (kedjeregel)

Residual:

$$
e_i = y_i - \hat y_i
$$

F√∂rlust i residualform:

$$
\mathcal{L}_{\text{MSE}}=\frac{1}{N}\sum_{i=1}^{N} e_i^{\,2}
$$

Derivata av yttre kvadrat:

$$
\frac{\partial}{\partial e_i}\bigl(e_i^{\,2}\bigr)=2e_i
$$

Residual vs. output:

$$
\frac{\partial e_i}{\partial \hat y_i} = -1
$$

Sigmoidkedja:

$$
\frac{\partial \hat y_i}{\partial a} = \sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\cdot x_i
$$

$$
\frac{\partial \hat y_i}{\partial b} = \sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\cdot 1
$$

Kedjan ihop:  

$$
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial a}
=\frac{2}{N}\sum_{i=1}^{N}\bigl(\hat y_i-y_i\bigr)\,\sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\,x_i
$$

$$
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial b}
=\frac{2}{N}\sum_{i=1}^{N}\bigl(\hat y_i-y_i\bigr)\,\sigma(z_i)\bigl(1-\sigma(z_i)\bigr)
$$

<br />

### 7.5 Vad varje faktor betyder

- Residual:
$$
\hat y_i - y_i
$$

- Sigmoidens lutning:
$$
\sigma(z_i)\bigl(1-\sigma(z_i)\bigr)
$$

- Inmatningsskala (endast f√∂r \(a\)):
$$
x_i
$$

Runt 0.5 √§r sigmoidens lutning stor ‚Üí stark signal. N√§ra 0 eller 1 √§r den liten ‚Üí m√§ttnad.

<br />

### 7.6 Varf√∂r vi inte har en sluten l√∂sning

Parametrarna \(a,b\) finns **inne i** sigmoid i summorna. Att s√§tta gradienterna till noll f√∂renklas inte till slutna ekvationer (som i linj√§r regression).  
D√§rf√∂r m√•ste vi **iterera**:

Inl√§rningsuppdatering:

$$
a \leftarrow a - \eta\,\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial a}
$$

$$
b \leftarrow b - \eta\,\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial b}
$$

Upprepa tills f√∂rlusten slutar minska.







