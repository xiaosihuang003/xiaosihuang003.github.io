---
title: "Koneoppiminen 5: Todenn√§k√∂isyydet ja Bayesilainen ajattelu"
subtitle: "DATA.ML.100 ¬∑ Joni K√§m√§r√§inen ¬∑ Ma 8.9.2025 K1704 üòä"
date: "2025-09-11"
lang: "fi"
excerpt: "Frekventistisest√§ intuitiosta Bayesin s√§√§nt√∂√∂n: ehdolliset todenn√§k√∂isyydet, priorien ja likelihoodien ero, ML- ja MAP-p√§√§t√∂kset, Monty Hall, ei-transitiiviset nopat, pienten otosten sudenkuopat ja miksi l√§√§k√§rit ajattelevat kuin bayesilaiset."
tags: ["Joni K√§m√§r√§inen", "koneoppiminen", "todenn√§k√∂isyys", "Bayes"]
draft: false
---

## Alkutiedot ‚Äî k√§yt√§nn√∂n asiat ja KNN-huomio
- Opettajan **k-NN viiteimplementaatio** vaikuttaa vahvalta; jos harjoituksesi tarkkuus ei yll√§ samaan, kysy *Exercise 3* -kanavalla.  
- My√∂s **liukulukutyyppi ja tarkkuus** voivat vaikuttaa selv√§sti tuloksiin. Opetus: *numeriikka on t√§rke√§√§*.

---
## Osa 1 ‚Äî Bayesilainen todenn√§k√∂isyys ja p√§√§ttely 

## 1.1 Johdanto
T√§n√§√§n siirrymme deterministisist√§ malleista (kuten suoran sovittaminen ja virhefunktion minimointi) **todenn√§k√∂isyysperustaiseen p√§√§ttelyyn**. T√§m√§ luento esittelee **Bayesilaisen todenn√§k√∂isyyden** ‚Äì kehikon, joka mahdollistaa uskomusten p√§ivitt√§misen uuden todistusaineiston perusteella.

Kaksi klassista nyrkkis√§√§nt√∂√§ ongelmanratkaisussa:
1. Jos et ole varma, yrit√§ sovittaa suora ‚Üí yksinkertainen aloitusmalli  
2. M√§√§rit√§ virhemittari ‚Üí etsi ratkaisu, joka minimoi sen  

Nyt otamme k√§ytt√∂√∂n **Bayesilaisen l√§hestymistavan**: aloitetaan prioriuskomuksista, p√§ivitet√§√§n ne havainnoilla ja saadaan posterioriuskomukset.

---

## 1.2 Todenn√§k√∂isyyskertaus nopilla

![Taulumuistiinpanot](/images/docs/mlLecture5_Bayesianlearning/1.png)

Otetaan reilu kuusisivuinen noppa:

- Todenn√§k√∂isyys saada `1`:  
  $$P(X=1)=\frac{1}{6}$$  

- Todenn√§k√∂isyys saada kaksi per√§kk√§ist√§ `1`:  
  $$P(\text{kaksi ykk√∂st√§})=P(1)\cdot P(1)=\frac{1}{36}$$  

- Todenn√§k√∂isyys **ei** saada `1`:  
  $$P(X\neq 1)=1-\frac{1}{6}=\frac{5}{6}$$  

T√§m√§ on **klassista todenn√§k√∂isyytt√§**.

---

## 1.3 Frekventistinen vs Bayesilainen tulkinta

- **Frekventistinen tulkinta:**  
  Todenn√§k√∂isyys = tapahtuman suhteellinen osuus √§√§rett√∂m√§ss√§ m√§√§r√§ss√§ toistoja.  
  Esim. heitt√§m√§ll√§ noppaa 1000 kertaa saat noin 1/6 ykk√∂si√§.

- **Bayesilainen tulkinta:**  
  Todenn√§k√∂isyys = *uskomuksen aste*, joka voidaan p√§ivitt√§√§ uudella tiedolla.  
  Voimme antaa todenn√§k√∂isyyksi√§ ja p√§ivitt√§√§ niit√§, vaikka meill√§ ei olisi paljon havaintoja.

---

## 1.4 Esimerkki: Monty Hall -ongelma

![Taulumuistiinpanot](/images/docs/mlLecture5_Bayesianlearning/2.png)

Peli:
- 3 ovea  
- Yhden takana auto, kahden takana vuohi  
- Valitset yhden oven  
- Juontaja avaa toisen oven (aina vuohen)  
- Sinulta kysyt√§√§n: **vaihdatko vai j√§√§tk√∂?**

Analyysi:
- Todenn√§k√∂isyys ett√§ alkuper√§inen valinta on oikea:  
  $$P(\text{auto valitun oven takana})=\frac{1}{3}$$  

- Todenn√§k√∂isyys ett√§ auto on muiden ovien takana:  
  $$P(\text{auto muiden ovien takana})=\frac{2}{3}$$  

Kun juontaja avaa yhden vuohen oven, **koko $2/3$ todenn√§k√∂isyys siirtyy viimeiselle suljetulle ovelle**.

Siis:
- J√§√§minen = voittotodenn√§k√∂isyys $1/3$  
- Vaihtaminen = voittotodenn√§k√∂isyys $2/3$  

**Johtop√§√§t√∂s:** Kannattaa aina vaihtaa ‚Äì voittotodenn√§k√∂isyys kaksinkertaistuu.

---

## 1.5 Ehdollinen todenn√§k√∂isyys

![Taulumuistiinpanot](/images/docs/mlLecture5_Bayesianlearning/3.png)

M√§√§ritelm√§:

$$
P(A\mid B)=\frac{P(A\cap B)}{P(B)}
$$

Tulkinta: todenn√§k√∂isyys ett√§ $A$ tapahtuu, kun tiedet√§√§n ett√§ $B$ on totta.

**Esimerkki 1:**  
- $A$: hahmo on hobitti  
- $B$: havaittu pituus = 120 cm  

Silloin:  
$$P(\text{hobitti}\mid \text{pituus}=120)$$ on suuri,  
kun taas $$P(\text{haltia}\mid \text{pituus}=120)$$ on pieni.

---

### Esimerkki 2: Nopan havainnointi p√§ivitt√§√§ uskomusta

Jos heit√§t noppaa ja **juuri ennen kuin se pys√§htyy**, n√§et ett√§ yl√§puolella on `5`.  
Mik√§ on todenn√§k√∂isyys, ett√§ lopullinen yl√§puoli on `1`?

- Vastakkaisten sivujen summa on 7 ‚Üí `1`:n vastakkainen on `6`  
- Jos yl√§puoli on `5`, nelj√§ viereist√§ sivua ovat yht√§ todenn√§k√∂isi√§ tulla yl√∂s viimeisen kier√§hdyksen j√§lkeen  

Siis:

$$
P(\text{yl√§=1}\mid \text{havaittu=5})=\frac{1}{4}
$$

Jos havaitsisimme yl√§puolen olevan `6`:

$$
P(\text{yl√§=1}\mid \text{havaittu=6})=0
$$

**Havainto muuttaa prioria.**  
Alkuper√§inen uskomus $P(\text{yl√§=1})=\frac{1}{6}$ t√§ytyy p√§ivitt√§√§. T√§m√§ on Bayesilaisen p√§√§ttelyn ydin:

> **Havainnot s√§√§t√§v√§t alkuper√§ist√§ uskomusta (prioria).**

---

## 1.6 Bayesin kaava

![Taulumuistiinpanot](/images/docs/mlLecture5_Bayesianlearning/4.png)

Bayesilaisen oppimisen perusta:

$$
P(A\mid B)=\frac{P(B\mid A)P(A)}{P(B)}
$$

Miss√§:
- $P(A)$ = prior (ennen havaintoja)  
- $P(B\mid A)$ = likelihood (todenn√§k√∂isyys jos $A$ on tosi)  
- $P(B)$ = marginaalinen todenn√§k√∂isyys  
- $P(A\mid B)$ = posterior (p√§ivitetty uskomus)

---

### Hobitti vs Haltia -esimerkki
- $A$: hahmo on hobitti tai haltia  
- $B$: havaittu pituus  

Posterior:
$$
P(\text{hobitti}\mid\text{pituus})=\frac{P(\text{pituus}\mid\text{hobitti})P(\text{hobitti})}{P(\text{pituus})}
$$

---

## 1.7 Priorin merkitys

![Taulumuistiinpanot](/images/docs/mlLecture5_Bayesianlearning/5.png)

Bayesin kaava muistuttaa: **priorilla on v√§li√§**.

$$
P(B)=\sum_A P(B\mid A)P(A)
$$

T√§ten  

$$
\sum_A P(A\mid B)=1
$$

Priorit vaikuttavat lopputulokseen.  
Esim. Mars-m√∂nkij√§n laskeutuminen ‚Üí prior arviona kivikkoisuuden todenn√§k√∂isyydelle vaikuttaa lopulliseen p√§√§t√∂kseen.

---

## Osa 2 ‚Äî Likelihoodien estimointi datasta

![Taulumuistiinpanot](/images/docs/mlLecture5_Bayesianlearning/6.png)

Bayesilaisessa p√§√§ttelyss√§ $P(B\mid A)$ on yleens√§ **tiheysfunktio**.  
Esim. pituuden $h$ jakaumat:
$$
\int_{-\infty}^{+\infty}p(h\mid\text{hobitti})\,\mathrm dh=1
$$

### 2.1 Histogrammit
Datan binnaus:
$$
\hat P(h\in I_j)=\frac{c_j}{N}
$$

### 2.2 Jatkuvan jakauman sovitus
Jos pituudet $\sim \mathcal N(\mu,\sigma^2)$:
$$
p(h\mid \mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}\exp\!\left(-\frac{(h-\mu)^2}{2\sigma^2}\right)
$$

> PDF-arvo ei ole todenn√§k√∂isyys ‚Äî vain integraali antaa todenn√§k√∂isyyden.

### 2.3 Parametrien estimaatti
$$
\hat\mu_{\text{ML}}=\frac{1}{N}\sum h_i,\quad
\hat\sigma^2_{\text{ML}}=\frac{1}{N}\sum (h_i-\hat\mu_{\text{ML}})^2
$$

### 2.4 Miksi $P(B\mid A)$ on helpompi mitata
Mittaa ensin luokkakohtaiset jakaumat, laske posteriorit Bayesin kaavalla.

---

## Osa 3 ‚Äî P√§√§t√∂kset: ML vs MAP

### 3.1 ML-p√§√§t√∂s
$$
\hat y_{\text{ML}}=\arg\max_{y\in\mathcal A}P(x\mid y)
$$

### 3.2 MAP-p√§√§t√∂s
$$
\hat y_{\text{MAP}}=\arg\max_{y\in\mathcal A}P(x\mid y)P(y)
$$

![Taulumuistiinpanot](/images/docs/mlLecture5_Bayesianlearning/7.png)

### 3.3 Normalisointi (kahden luokan tapaus)
$$
P(x)=P(x\mid H)P(H)+P(x\mid E)P(E)
$$

### 3.4 Miksi l√§√§k√§rit ovat ‚Äúluontaisesti bayesilaisia‚Äù
MAP huomioi priorin, ML ei ‚Üí siksi l√§√§k√§rit ensin ep√§ilev√§t yleisi√§ syit√§ ennen harvinaisia.

---
