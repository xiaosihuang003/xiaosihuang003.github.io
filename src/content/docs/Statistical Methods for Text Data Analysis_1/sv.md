---
title: "Statistiska metoder f√∂r textdataanalys_1"
subtitle: "(2 september 2025): Introduktion och grunder üòä"
date: 2025-09-04
lang: sv
excerpt: "Kurs√∂versikt, praktiska saker, varf√∂r textanalys √§r viktigt, sannolikhets- och ML-grunder samt Python/NLP-verktygsl√•dan."
tags: ["Jaakko Peltonen","Statistical Methods for Text Data Analysis","Chain rule"]
draft: false 
---

##  Introduktion & grunder

**Kursm√•l:** L√§ra oss representera, modellera och analysera text med statistiska metoder. F√∂rst bygga en solid grund, senare koppla till modern deep NLP (embeddingar, transformer).  
**Varf√∂r det √§r viktigt:** Text finns √∂verallt (nyheter, recensioner, sociala medier, lagar, vetenskap, transkript‚Ä¶). Statistiska metoder hj√§lper hantera brus, skala bortom manuell l√§sning och st√∂dja evidensbaserade slutsatser.  
**L√§randem√•l:** Bekanta oss med viktiga representationer (vektorrumsmodeller, neurala embeddingar), pipelines (tokenisering ‚Üí lemmatisering ‚Üí features), och modeller (n-gram, topic models, HMM, PCFG). Kunna sj√§lv till√§mpa k√§rntekniker.  
<br />

---

## 1. Praktiskt & deltagande

### 1.1 F√∂rel√§sningar
**Denna vecka:** p√• plats.  
**Kommande veckor:** via Zoom (p.g.a. resa). L√§nkar p√• Moodle.  
**Inspelningar:** laddas upp efter varje f√∂rel√§sning (l√§tt editerade). Bra f√∂r repetition, men live √§r b√§st.

### 1.2 Interaktivitet
Fr√•ga under f√∂rel√§sning n√§r n√•t √§r oklart. Anv√§nd Moodle-diskussion s√• alla f√•r nytta av svaren (men uppgifterna ska du g√∂ra sj√§lv).

### 1.3 Material & √∂vningar
Slides finns p√• Moodle. √ñvningar publicerade (PDF + inl√§mning).  
Deadline ‚âà 2 veckor. Sena inl√§mningar kan ge minuspo√§ng eller nekas.  
**Tips:** Jupyter Notebooks g√•r bra men kan bli r√∂riga. L√§mna ocks√• en ren PDF och g√§rna en separat `.py`.

### 1.4 Bed√∂mning & betyg
**Tentamen:** efter sista f√∂rel√§sningen, troligen i slutet av december. Omtentor p√• v√•ren och sommaren.  
**Viktning:** slutbetyg = $0.65 \times$ tentamen $+ 0.35 \times$ √∂vningar.  
**Gr√§nser:** godk√§nt runt 40 po√§ng, toppbetyg n√§ra 57 i √∂vningar. Tentamen: 5 problem √ó 10 po√§ng.

### 1.5 Relaterade kurser
Rule-based NLP, Deep NLP, Deep Learning.  
Informationss√∂kning och taligenk√§nning √§r relaterade men inte i fokus.  
<br />

---

## 2. Text & AI

- Tolkning beror p√• kontext, saknade ord och bias (Carson, Orwell, Richelieu).  
- M√§nniska‚Äìmaskin-kommunikation f√∂renklas ofta (korta queries, SEO-taggar).  
- Skala hj√§lper men long-tail-problem kvarst√•r. Ansvar och noggrannhet √§r viktiga.

### 2.1 Klassiska tankeexperiment
**Turing-testet:** om domare inte kan skilja m√§nniska fr√•n maskin i samtal, maskinen ‚Äúklarar testet‚Äù.  
**Kinesiska rummet (Searle):** klara samtal ‚â† f√∂rst√•else.

### 2.2 Moderna chatbots och LLM
M√•nga modeller (OpenAI, Google, Meta, Anthropic). Kursen fokuserar p√• gemensamma grunder.

### 2.3 Kompression & ‚Äúf√∂rst√•else‚Äù
**Hutter Prize:** b√§ttre kompression = b√§ttre modellering = framsteg i f√∂rst√•else.  
<br />

---

## 3. Engelska & spr√•klig variation

Ordklasser, morfologi, syntax, semantik, pragmatik.  
Spr√•k f√∂r√§ndras: stavning, grammatik, betydelser (t.ex. *nice*, *fantastic*, *meat*, *guy*).  
Fokus p√• engelska, men tekniker generaliseras.  
<br />

---

## 4. Sannolikhetsgrunder

Slumpvariabler med versaler ($X,Y$), v√§rden med gemener.

- Diskret: $0 \leq P(x)\leq 1$, $\sum_x P(x)=1$  
- Kontinuerlig: $p(x)\geq0$, $\int p(x)\,dx=1$  
- Gemensam: $P(x,y)$  
- Marginal: $P(y)=\sum_x P(x,y)$ eller $P(y)=\int p(x,y)\,dx$  
- Villkorlig: $P(y\mid x)=\dfrac{P(x,y)}{P(x)}$  
- Kedjeregeln:  
  $$P(a,\dots,z)=P(a\mid\dots,z)\cdot P(b\mid\dots,z)\cdots P(y\mid z)\cdot P(z)$$  
- Bayes:  
  $$P(y\mid x)=\dfrac{P(x\mid y)P(y)}{P(x)}$$  

**Exempel:** om $X=$ morgonv√§der, $Y=$ kv√§llsv√§der,  
$$P(Y=\text{rain}\mid X=\text{sun})=\dfrac{P(X=\text{sun},Y=\text{rain})}{P(X=\text{sun})}$$  
<br />

---

## 5. Maskininl√§rningens grunder

### 5.1 Data
Observationer + features (bag-of-words, embeddingar). Train/test-split, cross-validation.  
Struktur: sekvenser, hierarkier, flera korpusar.

### 5.2 Uppgifter
**Supervised:** klassificering, regression.  
**Unsupervised:** klustring, topic modeling, visualisering.

### 5.3 Utv√§rdering
- Regression: $$MSE=\frac{1}{N}\sum_i(y_i-\hat y_i)^2$$  
- Klassificering: $$\text{Error rate}=\frac{1}{N}\sum_i1(\hat y_i\neq y_i)$$  

### 5.4 Optimering
Gradient upp/ner:  
$$u \leftarrow u \pm \eta \nabla_u L$$  
Algoritmer: SGD, momentum, Adam, LBFGS.

### 5.5 Probabilistisk modellering
- Maximum likelihood: $$\hat\theta=\arg\max_\theta P(D\mid\theta)$$  
- MAP: $$\hat\theta=\arg\max_\theta P(D\mid\theta)P(\theta)$$  
- Posterior sampling: $\theta^{(s)}\sim P(\theta\mid D)$ via MCMC (MH, Gibbs).  
<br />

---

## 6. Fr√•n text till siffror

Process: normalisering ‚Üí tokenisering ‚Üí lemmatisering/stemming ‚Üí kollokationer ‚Üí features.  
Representationer: bag-of-words, TF-IDF, embeddingar.  
Modeller: n-gram, topic models, HMM, PCFG, klustring, visualisering. Senare neurala embeddingar och transformer.  
<br />

---

## 7. Verktygsl√•da

### 7.1 Milj√∂
Python ‚â• 3.9 rekommenderas. Anaconda f√∂r paket. Spyder liknar Matlab. Vilken IDE som helst funkar.

### 7.2 Installera paket
**pip:**  
```bash
    python -m pip install nltk numpy pandas matplotlib scikit-learn beautifulsoup4 scrapy
```
**conda:**  
```bash
    conda install nltk numpy pandas matplotlib scikit-learn
    conda install -c conda-forge beautifulsoup4 scrapy
```
### 7.3 Anv√§nda NLTK-resurser  
```bash
    import nltk
    nltk.download("punkt")
    nltk.download("wordnet")
    nltk.download("averaged_perceptron_tagger")
    nltk.download("stopwords")
```
### 7.4 Vanliga bibliotek
K√§rna: os, pickle, math, statistics, random, datetime, gzip  
Numeriskt: numpy, scipy  
Data: pandas, csv  
Plot: matplotlib  
Webb: scrapy, beautifulsoup4  
NLP: nltk  
Deep learning senare: torch, tensorflow, keras  
<br />
---

## 8. Python-snabbguide
```bash
**Aritmetik:** `+ - * / % **`  
**Variabler:** int, float, string, bool  
**Inspektera:** `dir()`, `type(x)`, `help(obj)`  
**Styrfl√∂de:**  

    if cond:
        ...
    elif cond2:
        ...
    else:
        ...

    for i in range(n):
        ...
    while cond:
        ...
```
**Datastrukturer:**  
Tuple `(1,2,"√§pple")` (of√∂r√§nderlig)  
List `[1,2,3]` (f√∂r√§nderlig)  
Dict `{"namn":"A Beautiful Mind","√•r":2001}`  
NumPy array `np.array([[1,2,3],[4,5,6]])`  
<br />

---

## 9. Studietips

G√∂r √∂vningarna i tid. Anv√§nd Moodle-diskussionerna. H√•ll ett rent repo: r√•data, notebooks, kod och rapporter separat. √ñvningarna √§r b√§sta f√∂rberedelse f√∂r tentan.  
<br />

---

## 10. Vad h√§nder n√§sta g√•ng

N√§sta g√•ng: praktisk textf√∂rbehandling, distributioner, kollokationer, vektorrum, klustring, n-gram-modeller; senare topic models och sekvensmodeller.  
<br />

---

## 11. √ñvning 1.3 ‚Äî Kedjeregeln: fullst√§ndig h√§rledning

En sekvens slumpvariabler $w_1,\dots,w_N$. F√∂r varje position $i$:  
v√§nster kontext $Left_i=[w_1,\dots,w_{i-1}]$, h√∂ger kontext $Right_i=[w_{i+1},\dots,w_N]$.  
Tom kontext reduceras till marginal. S√§tt $p(\emptyset)=1$.

Vi vill visa:

$$
\prod_{i=1}^{N}\frac{p(w_i\mid Left_i)}{p(w_i\mid Right_i)}=1.
$$

---

### Bevis A: b√•da produkterna = samma joint

1. Fram√•t:

$$
p(w_1,\dots,w_N)
= p(w_1)\,p(w_2\mid w_1)\cdots p(w_N\mid w_1,\dots,w_{N-1})
= \prod_{i=1}^{N} p(w_i\mid Left_i).
$$

2. Bak√•t:

$$
p(w_1,\dots,w_N)
= p(w_N)\,p(w_{N-1}\mid w_N)\cdots p(w_1\mid w_2,\dots,w_N)
= \prod_{i=1}^{N} p(w_i\mid Right_i).
$$

3. Kvoten:

$$
\prod_{i=1}^{N}\frac{p(w_i\mid Left_i)}{p(w_i\mid Right_i)}
= \frac{p(w_1,\dots,w_N)}{p(w_1,\dots,w_N)}=1.
$$

---

### Bevis B: teleskop via Bayes

F√∂r v√§nster kontext:

$$
p(w_i\mid Left_i)=\frac{p(w_1,\dots,w_i)}{p(w_1,\dots,w_{i-1})}.
$$

F√∂r h√∂ger kontext, l√•t $R_i=(w_i,\dots,w_N)$, och $Right_i=R_{i+1}$:

$$
p(w_i\mid Right_i)=\frac{p(w_i,\dots,w_N)}{p(w_{i+1},\dots,w_N)}.
$$

Allts√•:

$$
\frac{p(w_i\mid Left_i)}{p(w_i\mid Right_i)}
=\frac{p(w_1,\dots,w_i)\,p(w_{i+1},\dots,w_N)}
{p(w_1,\dots,w_{i-1})\,p(w_i,\dots,w_N)}.
$$

Multiplicera $i=1\dots N$:

$$
\prod_{i=1}^{N}\frac{p(w_i\mid Left_i)}{p(w_i\mid Right_i)}
=\frac{\prod_{i=1}^{N}p(w_1,\dots,w_i)\;\prod_{i=1}^{N}p(w_{i+1},\dots,w_N)}
{\prod_{i=1}^{N}p(w_1,\dots,w_{i-1})\;\prod_{i=1}^{N}p(w_i,\dots,w_N)}.
$$

Produkterna teleskoperar. Eftersom $p(\emptyset)=1$ f√•r vi:

$$
\prod_{i=1}^{N}\frac{p(w_i\mid Left_i)}{p(w_i\mid Right_i)}
=\frac{p(w_1,\dots,w_N)}{p(w_1,\dots,w_N)}=1.
$$

Bevis klart.  
<br />

---

## 12. Slides-recap & koncept

![Board notes](/images/docs/Statistical%20Methods%20for%20Text%20Data%20Analysis_1/1.png)

### 12.1 Koncept

Summan eller integralen = 1:

$$
\sum_k p(x=k)=1 \qquad \int p(y)\,dy=1
$$

Marginalisering:

$$
\sum_k p(x=k,y)=p(y)
$$

Villkorlig sannolikhet:

$$
p(y\mid x)=\frac{p(y,x)}{p(x)}
$$

Kedjeregel:

$$
p(a,b,c,\dots,y,z)=p(a\mid b,\dots,y,z)\,p(b\mid c,\dots,y,z)\cdots p(y\mid z)\,p(z)
$$

Bayes:

$$
p(y\mid x)=\frac{p(x\mid y)p(y)}{p(x)}
$$

### 12.2 Kopplingen till √∂vning 1.3

$\prod_{i=1}^{N}p(w_i\mid Left_i)$ = kedjeregel fram√•t,  
$\prod_{i=1}^{N}p(w_i\mid Right_i)$ = kedjeregel bak√•t.  
Kvoten blir 1, precis som vi visade.  
<br />

---

## Appendix A ‚Äî Viktiga formler

$$P(y)=\sum_x P(x,y)$$  
$$P(y\mid x)=\frac{P(x,y)}{P(x)}$$  
$$P(a,\dots,z)=P(a\mid\dots,z)\cdot\dots\cdot P(y\mid z)\cdot P(z)$$  
$$P(y\mid x)=\frac{P(x\mid y)P(y)}{P(x)}$$  

$$MSE=\frac{1}{N}\sum_i(y_i-\hat y_i)^2$$  
$$\text{Error rate}=\frac{1}{N}\sum_i1(\hat y_i\neq y_i)$$  

$$u\leftarrow u+\eta\nabla_u L\quad(\text{max})$$  
$$u\leftarrow u-\eta\nabla_u L\quad(\text{min})$$  

$$\hat\theta_{MLE}=\arg\max_\theta P(D\mid\theta)$$  
$$\hat\theta_{MAP}=\arg\max_\theta P(D\mid\theta)P(\theta)$$  
