---
title: "Tilastolliset menetelm√§t tekstidatan analysointiin_1"
subtitle: "(2. syyskuuta 2025): Johdanto ja perusteet üòä"
date: 2025-09-04
lang: fi
excerpt: "Kurssin yleiskatsaus, k√§yt√§nn√∂n asiat, miksi tekstianalyysi on t√§rke√§√§, todenn√§k√∂isyyden ja koneoppimisen perusteet sek√§ Python/NLP-ty√∂kalupakki."
tags: ["Jaakko Peltonen","Statistical Methods for Text Data Analysis","Chain rule"]
draft: false 
---

##  Johdanto & perusteet

**Kurssin tavoite:** Oppia esitt√§m√§√§n, mallintamaan ja analysoimaan teksti√§ tilastollisin menetelmin. Ensin rakennetaan vahva pohja, my√∂hemmin linkitet√§√§n moderniin deep NLP:hen (embeddingit, transformerit).  
**Miksi t√§m√§ on t√§rke√§√§:** Teksti on kaikkialla (uutiset, arviot, some, laki, tiede, litteraatit‚Ä¶). Tilastolliset menetelm√§t auttavat k√§sittelem√§√§n kohinaa, skaalaamaan manuaaliluvun yli ja tekem√§√§n p√§√§telmi√§ datan pohjalta.  
**Oppimistulokset:** Tutuiksi tulevat esitykset (vektorimallit, neuroembeddingit), prosessit (tokenisointi ‚Üí lemmatisointi ‚Üí piirteet), mallit (n-grammit, aihe¬≠mallit, HMM:t, PCFG:t). Osaat itse soveltaa ydintekniikoita.  
<br />

---

## 1. K√§yt√§nn√∂n asiat & osallistuminen

### 1.1 Tapaamiset
**T√§ll√§ viikolla:** kampuksella.  
**Seuraavat pari viikkoa:** Zoom (matkan takia). Linkit Moodlessa.  
**Tallenteet:** julkaistaan luennon j√§lkeen (pient√§ editointia). Hyvi√§ kertaamiseen, mutta suositellaan live-osallistumista.

### 1.2 Vuorovaikutus
Kysy luennolla heti jos jokin ep√§selv√§√§. Moodle-keskustelut hy√∂dytt√§v√§t kaikkia (mutta tee teht√§v√§t itse).

### 1.3 Materiaalit & harjoitukset
Diat Moodlessa. Harjoitukset julkaistu (PDF + palautus).  
Deadline yleens√§ ‚âà 2 viikkoa. My√∂h√§stymiset voi hyl√§t√§ tai v√§hent√§√§ pisteit√§.  
**Vinkki:** Jupyter Notebook k√§y, mutta voi olla sotkuinen. Palauta my√∂s siisti PDF ja mielell√§√§n erillinen `.py`.

### 1.4 Arviointi
**Tentti:** viimeisen luennon j√§lkeen, todenn√§k√∂isesti joulukuussa. Uusinnat kev√§√§ll√§ ja kes√§ll√§.  
**Painotus:** lopullinen arvosana = $0.65 \times$ tentti $+ 0.35 \times$ harjoitukset.  
**Rajapisteet:** l√§pi n. 40 pistett√§, ylin arvosana l√§hell√§ 57:√§√§ harjoituksissa. Tentiss√§ 5 teht√§v√§√§ √ó 10 pistett√§.

### 1.5 Sukulaiskurssit
Rule-based NLP, Deep NLP, Deep Learning.  
Tietohaku ja puheentunnistus sukua mutta eiv√§t fokuksessa.  
<br />

---

## 2. Teksti & teko√§ly

- Tulkinta riippuu kontekstista, puuttuvista sanoista ja biasista (Carson, Orwell, Richelieu).  
- Ihmisen‚Äìkoneen viestint√§ usein yksinkertaistetaan (lyhyet haut, SEO-tagit).  
- Skaala auttaa mutta long-tail-ongelmat s√§ilyv√§t. Vastuu ja tarkkuus v√§ltt√§m√§tt√∂mi√§.

### 2.1 Klassiset ajatuskokeet
**Turingin testi:** jos tuomari ei erota ihmist√§ koneesta, kone l√§p√§isee.  
**Kiinalainen huone:** keskustelun l√§p√§isy ‚â† ymm√§rt√§minen.

### 2.2 Modernit chatbotit ja LLM:t
Useita malleja (OpenAI, Google, Meta, Anthropic). Kurssilla keskityt√§√§n yhteisiin perusteisiin.

### 2.3 Pakkaus ja ‚Äúymm√§rrys‚Äù
**Hutter Prize:** parempi pakkaus ‚Üí parempi mallinnus ‚Üí enemm√§n ymm√§rryst√§.  
<br />

---

## 3. Englanti & kielen vaihtelu

Sanaluokat, morfologia, syntaksi, semantiikka, pragmatiikka.  
Kieli muuttuu: kirjoitusasu, kielioppi, merkitys (esim. *nice*, *fantastic*, *meat*, *guy*).  
Fokus englannissa, tekniikat yleistyv√§t muihin kieliin.  
<br />

---

## 4. Todenn√§k√∂isyyden perusteet

Satunnaismuuttujat isoilla ($X, Y$), arvot pienill√§.

- Diskreetti: $0 \leq P(x) \leq 1$, $\sum_x P(x) = 1$  
- Jatkuva: $p(x)\geq0$, $\int p(x)\,dx=1$  
- Yhteinen: $P(x,y)$  
- Marginaali: $P(y)=\sum_x P(x,y)$ tai $P(y)=\int p(x,y)\,dx$  
- Ehto: $P(y\mid x)=\dfrac{P(x,y)}{P(x)}$  
- Ketjus√§√§nt√∂:  
  $$P(a,\dots,z)=P(a\mid\dots,z)\cdot P(b\mid\dots,z)\cdots P(y\mid z)\cdot P(z)$$  
- Bayes:  
  $$P(y\mid x)=\dfrac{P(x\mid y)P(y)}{P(x)}$$  

**Esim:** jos $X=$ aamun s√§√§, $Y=$ illan s√§√§,  
$$P(Y=\text{rain}\mid X=\text{sun})=\dfrac{P(X=\text{sun},Y=\text{rain})}{P(X=\text{sun})}$$  
<br />

---

## 5. Koneoppimisen perusteet

### 5.1 Data
Havainnot + piirteet (bag-of-words, embeddingit). Train/test-jako, cross-validation.  
Rakenne: sekvenssit, hierarkiat, korpukset.

### 5.2 Teht√§v√§t
**Supervised:** luokittelu, regressio.  
**Unsupervised:** klusterointi, aihemallit, visualisointi.

### 5.3 Arviointi
- Regressio: $$MSE=\frac{1}{N}\sum_i(y_i-\hat y_i)^2$$  
- Luokittelu: $$\text{Error rate}=\frac{1}{N}\sum_i 1(\hat y_i\neq y_i)$$  

### 5.4 Optimointi
Gradientti yl√∂s/alas:  
$$u \leftarrow u \pm \eta \nabla_u L$$  
Algoritmit: SGD, momentum, Adam, LBFGS.

### 5.5 Probabilistinen mallinnus
- Maksimitodenn√§k√∂isyys: $$\hat\theta=\arg\max_\theta P(D\mid\theta)$$  
- MAP: $$\hat\theta=\arg\max_\theta P(D\mid\theta)P(\theta)$$  
- Posteriorin√§ytteet: $\theta^{(s)}\sim P(\theta\mid D)$ MCMC:ll√§ (MH, Gibbs).  
<br />

---

## 6. Tekstist√§ numeroiksi

Prosessi: normalisointi ‚Üí tokenisointi ‚Üí lemmatisointi/stemm√§ys ‚Üí kollokaatiot ‚Üí piirteet.  
Esitykset: bag-of-words, TF-IDF, embeddingit.  
Mallit: n-grammit, aihemallit, HMM, PCFG, klusterointi, visualisointi. My√∂hemmin neuroembeddingit ja transformerit.  
<br />

---

## 7. Ty√∂kalupakki

### 7.1 Ymp√§rist√∂
Python ‚â• 3.9 suositus. Anaconda helpottaa paketteja. Spyder muistuttaa Matlabi√§. Mik√§ tahansa IDE k√§y.

### 7.2 Pakettien asennus
**pip:**  
```bash
    python -m pip install nltk numpy pandas matplotlib scikit-learn beautifulsoup4 scrapy
```
**conda:**  
```bash
    conda install nltk numpy pandas matplotlib scikit-learn
    conda install -c conda-forge beautifulsoup4 scrapy
```
### 7.3 NLTK-resurssit  
```bash
    import nltk
    nltk.download("punkt")
    nltk.download("wordnet")
    nltk.download("averaged_perceptron_tagger")
    nltk.download("stopwords")
```
### 7.4 Kirjastot
Perus: os, pickle, math, statistics, random, datetime, gzip  
Num: numpy, scipy  
Data: pandas, csv  
Plottaus: matplotlib  
Web: scrapy, beautifulsoup4  
NLP: nltk  
Deep learning my√∂hemmin: torch, tensorflow, keras  
<br />
---

## 8. Python-pikamuistio
```bash
**Aritmetiikka:** `+ - * / % **`  
**Muuttujat:** int, float, string, bool  
**Tutki:** `dir()`, `type(x)`, `help(obj)`  
**Ohjausrakenne:**  

    if cond:
        ...
    elif cond2:
        ...
    else:
        ...

    for i in range(n):
        ...
    while cond:
        ...
```
**Tietorakenteet:**  
Tuple `(1,2,"omena")` (ei-muokattava)  
List `[1,2,3]` (muokattava)  
Dict `{"nimi":"A Beautiful Mind","vuosi":2001}`  
NumPy array `np.array([[1,2,3],[4,5,6]])`  
<br />

---

## 9. Opiskeluvinkit

Tee harjoitukset ajoissa. K√§yt√§ Moodle-keskusteluja. Pid√§ siisti repo: raakadata, notebookit, koodi ja raportit erill√§√§n. Harjoitukset valmistavat tenttiin.  
<br />

---

## 10. Mit√§ seuraavaksi

Seuraavaksi: k√§yt√§nn√∂n tekstin esik√§sittely, jakaumat, kollokaatiot, vektoriavaruudet, klusterointi, n-gram-mallit; my√∂hemmin aihemallit ja sekvenssimallit.  
<br />

---

## 11. Harjoitus 1.3 ‚Äî Ketjus√§√§nt√∂: t√§ydellinen todistus

Jono satunnaismuuttujia $w_1,\dots,w_N$. Jokaiselle $i$:lle vasen konteksti $Left_i=[w_1,\dots,w_{i-1}]$,  
oikea konteksti $Right_i=[w_{i+1},\dots,w_N]$.  
Tyhj√§ konteksti = marginaali. Olkoon $p(\emptyset)=1$.

Todistettava:

$$
\prod_{i=1}^{N}\frac{p(w_i\mid Left_i)}{p(w_i\mid Right_i)}=1.
$$

---

### Todistus A: molemmat tulot = sama yhteisjakauma

1. Ketjus√§√§nt√∂ eteenp√§in:

$$
p(w_1,\dots,w_N)
= p(w_1)\,p(w_2\mid w_1)\cdots p(w_N\mid w_1,\dots,w_{N-1})
= \prod_{i=1}^{N} p(w_i\mid Left_i).
$$

2. Ketjus√§√§nt√∂ taaksep√§in:

$$
p(w_1,\dots,w_N)
= p(w_N)\,p(w_{N-1}\mid w_N)\cdots p(w_1\mid w_2,\dots,w_N)
= \prod_{i=1}^{N} p(w_i\mid Right_i).
$$

3. Suhde:

$$
\prod_{i=1}^{N}\frac{p(w_i\mid Left_i)}{p(w_i\mid Right_i)}
= \frac{p(w_1,\dots,w_N)}{p(w_1,\dots,w_N)}=1.
$$

---

### Todistus B: teleskooppi Bayesin avulla

Vasemman kontekstin osalta

$$
p(w_i\mid Left_i)=\frac{p(w_1,\dots,w_i)}{p(w_1,\dots,w_{i-1})}.
$$

Oikean kontekstin osalta, $R_i=(w_i,\dots,w_N)$, $Right_i=R_{i+1}$:

$$
p(w_i\mid Right_i)=\frac{p(w_i,\dots,w_N)}{p(w_{i+1},\dots,w_N)}.
$$

Siten

$$
\frac{p(w_i\mid Left_i)}{p(w_i\mid Right_i)}
=\frac{p(w_1,\dots,w_i)\,p(w_{i+1},\dots,w_N)}
{p(w_1,\dots,w_{i-1})\,p(w_i,\dots,w_N)}.
$$

Kun kerrotaan $i=1\dots N$, saadaan

$$
\prod_{i=1}^{N}\frac{p(w_i\mid Left_i)}{p(w_i\mid Right_i)}
=\frac{\prod_{i=1}^{N}p(w_1,\dots,w_i)\;\prod_{i=1}^{N}p(w_{i+1},\dots,w_N)}
{\prod_{i=1}^{N}p(w_1,\dots,w_{i-1})\;\prod_{i=1}^{N}p(w_i,\dots,w_N)}.
$$

Telescoping supistaa kaiken pois: $p(\emptyset)=1$. Lopputulos:

$$
\prod_{i=1}^{N}\frac{p(w_i\mid Left_i)}{p(w_i\mid Right_i)}
=\frac{p(w_1,\dots,w_N)}{p(w_1,\dots,w_N)}=1.
$$

Q.E.D.  
<br />

---

## 12. Diakertaus ja k√§sitteet

![Board notes](/images/docs/Statistical%20Methods%20for%20Text%20Data%20Analysis_1/1.png)

### 12.1 K√§sitteet

Summa tai integraali = 1:

$$
\sum_k p(x=k)=1 \qquad \int p(y)\,dy=1
$$

Marginalisointi poistaa muuttujia:

$$
\sum_k p(x=k,y)=p(y)
$$

Ehtotodenn√§k√∂isyys yhdist√§√§ yhteis- ja marginaalin:

$$
p(y\mid x)=\frac{p(y,x)}{p(x)}
$$

Ketjus√§√§nt√∂:

$$
p(a,b,c,\dots,y,z)=p(a\mid b,\dots,y,z)\cdot p(b\mid c,\dots,y,z)\cdots p(y\mid z)\,p(z)
$$

Bayes:

$$
p(y\mid x)=\frac{p(x\mid y)p(y)}{p(x)}
$$

### 12.2 Harjoitus 1.3:n yhteys

$\prod_{i=1}^{N}p(w_i\mid Left_i)$ = ketjus√§√§nn√∂n hajotelma eteenp√§in,  
$\prod_{i=1}^{N}p(w_i\mid Right_i)$ = ketjus√§√§nn√∂n hajotelma taaksep√§in.  
Suhde = 1, kuten todistettiin.  
<br />

---

## Liite A ‚Äî Keskeiset kaavat

$$P(y)=\sum_x P(x,y)$$  
$$P(y\mid x)=\frac{P(x,y)}{P(x)}$$  
$$P(a,\dots,z)=P(a\mid\dots,z)\cdot\dots\cdot P(y\mid z)\cdot P(z)$$  
$$P(y\mid x)=\frac{P(x\mid y)P(y)}{P(x)}$$  

$$MSE=\frac{1}{N}\sum_i(y_i-\hat y_i)^2$$  
$$\text{Error rate}=\frac{1}{N}\sum_i1(\hat y_i\neq y_i)$$  

$$u\leftarrow u+\eta\nabla_u L\quad(\text{max})$$  
$$u\leftarrow u-\eta\nabla_u L\quad(\text{min})$$  

$$\hat\theta_{MLE}=\arg\max_\theta P(D\mid\theta)$$  
$$\hat\theta_{MAP}=\arg\max_\theta P(D\mid\theta)P(\theta)$$  
