---
title: "Lecture 3: 線性分類"
subtitle: "DATA.ML.100 · Joni Kämäräinen · 星期一 1.9.2025 · K1704"
date: 2025-09-02
lang: yue
excerpt: "我哋會先複習線性迴歸同簡單嘅基準分類，再進入分類問題，包括 k-NN（距離/k/計算）、線性分隔嘅視覺化、階躍函數、邏輯（sigmoid）輸出、MSE 梯度，仲有點解冇封閉解。最後會引入神經網絡。"
tags: ["Joni Kämäräinen","machine-learning","linear-classification","step-function", "sigmoid-function", "logistic"]
draft: false
---
<details>
<summary><strong>內容目錄（需要時點擊 ▶️）</strong></summary>

- [Part 1 : 上星期课程回顾](#part-1--上星期课程回顾)
  - [1.1 模型同训练数据](#11-模型同训练数据)
  - [1.2 误差同解法](#12-误差同解法)
  - [1.3 点解解析解咁重要](#13-点解解析解咁重要)
  - [1.4 属于咩类型问题](#14-属于咩类型问题)
- [Part 2 : 基线同分类](#part-2--基线同分类)
  - [2.1 一个简单嘅基线](#21-一个简单嘅基线)
  - [2.2 转去分类](#22-转去分类)
  - [2.3 输出类型改变](#23-输出类型改变)
  - [2.4 观测係乜嘢](#24-观测係乜嘢)
- [Part 3 : 例子：霍比特人 vs. 精灵](#part-3--例子霍比特人-vs-精灵)
  - [3.1 由训练样本学习](#31-由训练样本学习)
  - [3.2 半兽人嘅陷阱故事](#32-半兽人嘅陷阱故事)
  - [3.3 生成数据](#33-生成数据)
  - [3.4 接受重叠](#34-接受重叠)
  - [3.5 扩展到二维](#35-扩展到二维)
  - [3.6 训练 vs. 推断](#36-训练-vs-推断)
  - [3.7 第一个分类想法](#37-第一个分类想法)
- [Part 4 : 最近邻分类器](#part-4--最近邻分类器)
  - [4.1 名字同概念](#41-名字同概念)
  - [4.2 训练（储存晒所有嘢）](#42-训练储存晒所有嘢)
  - [4.3 推断（揾最近嘅抄佢标签）](#43-推断揾最近嘅抄佢标签)
  - [4.4 分类基线](#44-分类基线)
  - [4.5 总结](#45-总结)
- [Part 5 : k-NN 考虑 → 拟合直线](#part-5--k-nn-考虑--拟合直线)
  - [5.1 k-NN 入面可以调嘅嘢](#51-k-nn-入面可以调嘅嘢)
  - [5.2 可唔可以用拟合直线嚟分类？](#52-可唔可以用拟合直线嚟分类)
  - [5.3 分类误差点样量度？](#53-分类误差点样量度)
- [Part 6 : 由拟合直线到阶跃，再到 sigmoid](#part-6--由拟合直线到阶跃再到-sigmoid)
  - [6.1 阶跃规则同判别点](#61-阶跃规则同判别点)
  - [6.2 用 logistic（sigmoid）近似阶跃](#62-用-logisticsigmoid近似阶跃)
  - [6.3 暂时嘅训练信号](#63-暂时嘅训练信号)
- [Part 7 : Sigmoid 输出嘅 MSE 同梯度（逐步推导，清晰版）](#part-7--sigmoid-输出嘅-mse-同梯度逐步推导清晰版)
  - [7.1 模型、目标、损失](#71-模型目标损失)
  - [7.2 点解 ∂zᵢ/∂b = 1 同 ∂zᵢ/∂a = xᵢ？](#72-点解-∂zᵢ∂b--1-同-∂zᵢ∂a--xᵢ)
  - [7.3 Sigmoid 嘅导数](#73-sigmoid-嘅导数)
  - [7.4 对 MSE 求导（链式法则）](#74-对-mse-求导链式法则)
  - [7.5 各个因子意思](#75-各个因子意思)
  - [7.6 点解冇封闭解？](#76-点解冇封闭解)

</details>


## Part 1 : 上星期课程回顾

![Board notes](/images/docs/Lecture3_Linearclassification/1.png)

我哋由一个基本概念开始：一次观测可以写成函数关系 $y = f(x)$。咁样问题就变成咗机器学习：模型畀预测值 $\hat{y}$，再同真实值 $y$ 做比较。

<br />

### 1.1 模型同训练数据

我哋用嘅线性模型好简单，
$$
\hat{y} = a x + b ,
$$
入面 $a$ 同 $b$ 係要学习嘅参数。如果有 $N$ 个样本，训练数据就写成
$$
(x_i, y_i), \quad i = 1, \ldots, N .
$$

<br />

### 1.2 误差同解法

要拣 $a$ 同 $b$，我哋就要最小化预测同真实值之间嘅误差：
$$
\mathcal{L} = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 .
$$
之后令梯度等于零，就可以揾到最优解：
$$
\frac{\partial \mathcal{L}}{\partial a} = 0, \qquad
\frac{\partial \mathcal{L}}{\partial b} = 0 .
$$
咁样就会得到解析解（功课入面我哋都用过）。

<br />

### 1.3 点解解析解咁重要

解析解（**closed-form/闭式解**）即係可以直接写成公式嘅解，唔需要数值迭代。计算好快：代数据入去、做求和，几毫秒就得。对实时系统嚟讲（课堂举例係飞机系统一秒做几千次决策）特别关键。

<br />

### 1.4 属于咩类型问题

输出係实数 $y \in \mathbb{R}$，所以呢个係一个**回归问题**。回归问题好常见，之后做咩任务之前，通常都会先同线性回归比较。

---
## Part 2 : 基线同分类

![Board notes](/images/docs/Lecture3_Linearclassification/2.png)

<br />

### 2.1 一个简单嘅基线

要判断线性回归够唔够好，我哋先加咗一个好简单嘅基线：**乜都唔学**——直接预测所有训练目标嘅平均值。黑板上写成 “$\hat{y}$ = average of all training data $y_i$”，即係
$$
\hat{y}=\frac{1}{N}\sum_{i=1}^{N} y_i .
$$
呢个计算好容易，而且应该永远先做。线性回归应该明显好过呢个基线；如果唔係，就要检查数据或者流程。

<br />

### 2.2 转去分类

跟住就讲另一个重要问题：**分类**。主要意思係：直线拟合同样可以用喺分类度，但要加啲细微改动。

<br />

### 2.3 输出类型改变

呢度输出**唔係实数**，而係**离散标签**。黑板上写为
$$
y \in \{1,2,3\}.
$$
任务就係决定输入属于边个类别。

<br />

### 2.4 观测係乜嘢

$x$ 就係观测，可以係好多嘢：
- 图像（课堂例子成日用图像）；
- 温度；
- 声音；
- 或者呢啲嘅组合。

典型例子係自动驾驶：系统每秒捉好多次图像，要检测行人、车辆、单车，再将信息传畀驾驶系统。呢就係一个用离散标签嘅分类问题。

---
## Part 3 : 例子：霍比特人 vs. 精灵

![Board notes](/images/docs/Lecture3_Linearclassification/3.png)

<br />

### 3.1 由训练样本学习

我哋仲係用模型 $y = f(x)$，$f$ 嘅参数就係由训练样本学返嚟。呢部分同回归一样：收集数据，然后拟合函数。  

<br />

### 3.2 半兽人嘅陷阱故事

课堂讲咗一个故事。假设我哋係半兽人，有个陷阱。如果捉到霍比特人，就好嘢，可以食；但如果捉到精灵，就危险，会有麻烦。所以观测 $x$ 可以係生物嘅**身高**。  

假设霍比特人比较矮，精灵比较高。呢个就係我哋用嚟区分佢哋嘅特征。  

<br />

### 3.3 生成数据

为了模拟，假设中土世界嘅身高分布服从正态分布，就好似现实世界动物特征咁。我哋为霍比特人随机生成五个样本，为精灵生成五个样本。霍比特人用一种颜色，精灵用另一种颜色。  

样本入面可以见到有一个特别高嘅霍比特人，同一个比较矮嘅精灵。呢说明两个类别唔係完全分开嘅，会有重叠。现实世界都係咁，呢表示分类冇可能做到100%准确。  

<br />

### 3.4 接受重叠

因为有重叠，所以必须接受会有错误。无论建立咩系统，总会有啲错。  
- 例如：癌症检测，唔係所有癌症都检测得到，有时健康样本都会误判做癌症。  

所以分类任务必须接受会有一定错误率。  

<br />

### 3.5 扩展到二维

到目前为止，我哋只用咗一个特征：身高。但其实可以加多一个，例如体重。假设：霍比特人矮而重，精灵高而瘦。  

咁样我哋就有**二维数据**：  
- $x_1 =$ 身高  
- $x_2 =$ 体重  

而家每个训练点就係一个 $(x_1, x_2)$ 对应，$y$ 就係类别标签。训练数据入面可以用数值标签，例如 $y=0$ 表示霍比特人，$y=1$ 表示精灵。  

<br />

### 3.6 训练 vs. 推断

机器学习通常分为两个阶段：  
1. **训练** —— 用标注样本学习模型参数。  
2. **推断** —— 喺新样本上测试模型，判断类别。  

所以如果有一个新点输入，任务就係决定佢係霍比特人定精灵。  

<br />

### 3.7 第一个分类想法

课堂上黑板画咗一个新样本。学生思考点样分类。直觉答案係：判为霍比特人，因为佢离开霍比特人样本好近。  

呢就自然引出其中一个最早嘅分类方法：利用与训练样本嘅接近程度。

---

## Part 4 : 最近邻分类器

![Board notes](/images/docs/Lecture3_Linearclassification/4.png)

<br />

### 4.1 名字同概念
我哋由“接近”嘅直觉，转到具体方法：**最近邻分类器**（1-NN，之后推广到 $k$-NN）。概念好简单，新样本就跟最近嘅训练样本一个类别。

<br />

### 4.2 训练（储存晒所有嘢）
训练阶段好简单，**就係储存所有训练样本**（特征同类别标签）。呢阶段冇参数要拟合。

<br />

### 4.3 推断（揾最近嘅，抄佢标签）
当有新样本嚟到，就做以下计算：  
1. 对**所有**训练样本，计算同新样本嘅距离。  
   （乜距离度量都得，只要可以比较边个近就可以。）  
2. 如果距离**细过目前最好**，就更新最好结果，并**拣**呢个样本。  
3. **返回**最近距离嗰个样本嘅类别标签。  

呢就係成个算法。练习会用 Python 实现。

<br />

### 4.4 分类基线
我哋仲设定基线去判断分类器够唔够好：  
- **随机类别** —— 喺类别集合入面随机输出一个标签。  
- **最常见标签** —— 永远输出训练数据入面出现最多嘅标签。  

第二个基线通常好过随机，尤其当一个类别**占主导**（例如99%样本都係一个类别）。如果“最常见标签”已经有99%准确率，我哋方法必须超过**呢个数**先有意义。

<br />

### 4.5 总结
- 最近邻算法**非常简单**，但**出奇咁强大**。  
- 永远要同**简单基线**比较，可以睇到问题难度几高，同埋我哋必须超过嘅基准表现。

---

## Part 5 : k-NN 考虑 → 拟合直线

![Board notes](/images/docs/Lecture3_Linearclassification/5.png)

<br />

### 5.1 k-NN 入面可以调嘅嘢
当我哋开始研究最近邻度入面可以改咩时，就会打开一大片世界：

- **距离。** 可以拣唔同嘅距离度量。  
  一维嘅欧式距离係  
  $$
  d(x,x_i)=\sqrt{(x-x_i)^2},
  $$
  而**曼哈顿（L1）**式距离係  
  $$
  d(x,x_i)=|x-x_i|.
  $$
  仲有好多其他距离；应该拣啱数据嘅。

- **邻居数量。** $k$ 可以係 $1,3,\dots$  
  如果得两个类别，$k=2$ 会平手（一个霍比特人，一个精灵），所以用 $k=3$ 可以避免平手。

- **计算时间。** 基础 1-NN 要循环所有训练点，大数据集会慢，但有**快速最近邻方法**（通常係近似嘅），可以将空间分区，加快搜索。

- **1D vs 2D 入面嘅距离：**  
  喺 **1D**，欧式（L2）同曼哈顿（L1）其实一样，都变成  
  $$
  |x-x_i|.
  $$  
  喺 **2D 或更高维**，佢哋就唔同：  
  - 2D 欧式（L2）：  
    $$
    d(\mathbf{x},\mathbf{x_i})=\sqrt{(x_1-x_{i1})^2+(x_2-x_{i2})^2},
    $$
    结果係圆形邻域。  
  - 2D 曼哈顿（L1）：  
    $$
    d(\mathbf{x},\mathbf{x_i})=|x_1-x_{i1}|+|x_2-x_{i2}|,
    $$
    结果係菱形邻域。  

  呢个差异会改变 **k-NN 嘅决策边界**：用 L2 边界圆滑；用 L1 边界沿坐标轴。拣边个要睇数据几何形状。

**🤔 功课：** 给出一组二维点，让 **1-NN** 同 **3-NN** 得出**唔同**分类。

<br />

### 5.2 可唔可以用拟合直线嚟分类？
![Board notes](/images/docs/Lecture3_Linearclassification/6.png)

试吓将分类写成**拟合直线**。我哋定义类别目标为
$$
y=-1 \;\text{表示霍比特人}, \qquad y=+1 \;\text{表示精灵}.
$$
然后对 $(x,y)$ 点拟合一条直线
$$
\hat y = a x + b
$$
（完全同回归一样）。  
拟合完之后，用一个简单嘅**分类规则**：
$$
\hat y < 0 \Rightarrow \text{霍比特人}, \qquad
\hat y > 0 \Rightarrow \text{精灵}.
$$
当 $\hat y=0$ 嗰点就係**判别点**（决策边界）。

课堂例子入面，我哋拟合咗条直线，再测试几个新样本；根据呢组数据，上面嘅规则全部分得啱。

![Board notes](/images/docs/Lecture3_Linearclassification/7.png)

<br />

### 5.3 分类误差点样量度？
分类嘅误差係**二值**嘅：  
- 如果 $\hat y$ 预测**啱类别** ⇒ $\text{err}=0$；  
- 如果 $\hat y$ 预测**错类别** ⇒ $\text{err}=1$。  

冇“可能”输出。  
要注意：拟合直线最小化嘅係**平方误差**，唔係呢个 0/1 误差。呢个差异会之后带来问题——我哋下一步会处理。

---
## Part 6 : 由拟合直线到阶跃，再到 sigmoid

![Board notes](/images/docs/Lecture3_Linearclassification/8.png)

<br />

### 6.1 阶跃规则同判别点

保留拟合直线 $\hat y = a x + b$，喺 $\hat y=0$ 定义**判别点**。  
黑板上嘅阶跃规则用嘅係 **$\pm1$** 标签：

$$
y =
\begin{cases}
+1, & x > x_d,\\[4pt]
-1, & x < x_d .
\end{cases}
$$

等价于用直线符号：

$$
\hat y < 0 \Rightarrow \text{霍比特人}, \qquad
\hat y > 0 \Rightarrow \text{精灵}.
$$

![Board notes](/images/docs/Lecture3_Linearclassification/9.png)

<br />

### 6.2 用 logistic（sigmoid）近似阶跃

因为硬阶跃係唔连续嘅，我哋用佢平滑嘅近似函数 logistic（“logsig”）：

$$
\operatorname{logsig}(x)=\frac{1}{1+e^{-x}} .
$$

同直线组合：

$$
\hat y \;=\; \operatorname{logsig}(a x + b)
=\frac{1}{1+e^{-(a x + b)}} .
$$

当中 $a$ 控制过渡有几陡峭，$b$ 决定阈值位置。  
类别重标到 $[0,1]$ 区间（霍比特人 $=0$，精灵 $=1$）。

<br />

### 6.3 暂时嘅训练信号

有咗呢个平滑输出之后，可以继续最小化目标 $\{0,1\}$ 同 $\hat y$ 之间嘅**均方误差**，嚟拟合 $a$ 同 $b$。


---
## Part 7 : Sigmoid 输出嘅 MSE 同梯度（逐步推导，清晰版）

![Board notes](/images/docs/Lecture3_Linearclassification/10.png)

<br />

### 7.1 模型、目标、损失

模型（1D 特征 \(x\)）：

$$
z_i = a x_i + b
$$

$$
\hat y_i = \sigma(z_i) = \frac{1}{1+e^{-z_i}}
$$

目标：

$$
y_i \in \{0,1\}
$$

均方误差 (MSE)：

$$
\mathcal{L}_{\text{MSE}}=\frac{1}{N}\sum_{i=1}^{N}\bigl(y_i-\hat y_i\bigr)^2
$$

<br />

### 7.2 点解 $ \frac{\partial z_i}{\partial b} = 1 $ 同 $ \frac{\partial z_i}{\partial a} = x_i $？

因为条直线係：

$$
z_i = a x_i + b
$$

对 \(b\) 求导：

$$
\frac{\partial z_i}{\partial b} = 1
$$

对 \(a\) 求导：

$$
\frac{\partial z_i}{\partial a} = x_i
$$

即係：改 \(b\) 会上下平移直线，改 \(a\) 会令直线倾斜，幅度由 \(x_i\) 决定。

<br />

### 7.3 Sigmoid 嘅导数

定义：

$$
\sigma(z)=\frac{1}{1+e^{-z}}
$$

导数：

$$
\frac{d\sigma}{dz}=\sigma(z)\bigl(1-\sigma(z)\bigr)
$$

<br />

### 7.4 对 MSE 求导（链式法则）

残差：

$$
e_i = y_i - \hat y_i
$$

损失写成残差形式：

$$
\mathcal{L}_{\text{MSE}}=\frac{1}{N}\sum_{i=1}^{N} e_i^{\,2}
$$

外层平方求导：

$$
\frac{\partial}{\partial e_i}\bigl(e_i^{\,2}\bigr)=2e_i
$$

残差同输出嘅关系：

$$
\frac{\partial e_i}{\partial \hat y_i} = -1
$$

Sigmoid 链（同直线组合）：

$$
\frac{\partial \hat y_i}{\partial a} = \sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\cdot x_i
$$

$$
\frac{\partial \hat y_i}{\partial b} = \sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\cdot 1
$$

将链式拼埋一齐：  

$$
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial a}
=\frac{2}{N}\sum_{i=1}^{N}\bigl(\hat y_i-y_i\bigr)\,\sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\,x_i
$$

$$
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial b}
=\frac{2}{N}\sum_{i=1}^{N}\bigl(\hat y_i-y_i\bigr)\,\sigma(z_i)\bigl(1-\sigma(z_i)\bigr)
$$

<br />

### 7.5 各个因子意思

- 残差：
$$
\hat y_i - y_i
$$

- Sigmoid 斜率：
$$
\sigma(z_i)\bigl(1-\sigma(z_i)\bigr)
$$

- 输入缩放（只影响 \(a\)）：
$$
x_i
$$

Sigmoid 喺 0.5 附近斜率大（学习信号强）；接近 0 或 1 时斜率细（饱和）。

<br />

### 7.6 点解冇封闭解？

参数 \(a,b\) 喺 sigmoid 入面出现，令和式冇办法化简成封闭公式。唔似线性回归咁，可以直接解。  
所以必须**迭代**：

学习率更新：

$$
a \leftarrow a - \eta\,\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial a}
$$

$$
b \leftarrow b - \eta\,\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial b}
$$

重复直到损失唔再下降。

