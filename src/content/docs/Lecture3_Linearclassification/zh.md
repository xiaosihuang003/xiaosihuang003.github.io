---
title: "第三讲：线性分类"
subtitle: "DATA.ML.100 · Joni Kämäräinen · 2025年9月1日星期一 · K1704"
date: 2025-09-02
lang: zh
excerpt: "线性回归回顾 → 简单基线方法 → 分类问题。k-近邻法（距离/k值/复杂度），线性拟合视角，阶跃规则，逻辑（S型）函数输出，均方误差梯度，以及为什么没有解析解——为神经网络学习做准备。"
tags: ["线性分类", "k-近邻法", "基线方法", "阶跃函数", "S型函数", "逻辑回归"]
draft: false
---

## 第一部分：上周课程回顾

![板书笔记](/images/docs/Lecture3_Linearclassification/1.png)

我们从这样一个基本概念开始：一个观测结果可以表示为一个函数，$y = f(x)$。这对我们来说构成了一个机器学习问题：模型给出预测值 $\hat{y}$，我们将其与真实值 $y$ 进行比较。

<br />

### 1.1 模型与训练数据

我们使用了一个非常简单的线性模型：
$$
\hat{y} = a x + b ,
$$
其中 $a$ 和 $b$ 是待学习的参数。对于 $N$ 个样本，训练数据为：
$$
(x_i, y_i), \quad i = 1, \ldots, N .
$$

<br />

### 1.2 误差函数及其求解

为了确定 $a$ 和 $b$，我们最小化真实值与预测值之间的误差：
$$
\mathcal{L} = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 .
$$
然后我们将梯度设为零来求得最优解：
$$
\frac{\partial \mathcal{L}}{\partial a} = 0, \qquad
\frac{\partial \mathcal{L}}{\partial b} = 0 .
$$
这样就得到了一个解析解（我们在作业中也使用了这个方法）。

<br />

### 1.3 解析解的重要性

解析解的计算速度非常快：我们只需要代入数据点，进行求和运算，就能在毫秒级时间内得到估计值。这种速度在实时应用中非常重要（课堂例子中飞机系统需要每秒进行上千次旋转决策）。

<br />

### 1.4 问题类型

我们的输出是一个连续实数值，$y \in \mathbb{R}$，因此这是一个**回归问题**。回归问题在实践中无处不在，无论后续学习什么内容，我们都会首先与线性回归进行比较。

<br />

---

## 第二部分：基线方法与分类问题

![板书笔记](/images/docs/Lecture3_Linearclassification/2.png)

<br />

### 2.1 简单的基线方法

我们首先建立了一个非常简单的基线方法，用于判断线性回归模型是否有效。思路是：**不进行任何学习**只预测所有训练目标值的平均值。在黑板上写作“$\hat{y}$ = 所有训练数据 $y_i$ 的平均值”，即：
$$
\hat{y}=\frac{1}{N}\sum_{i=1}^{N} y_i .
$$
这种方法计算简单，我们应该始终建立这样的基线。我们的线性回归模型应该明显优于这个基线；如果不是，说明数据可能存在问题，需要进一步检查。

<br />

### 2.2 转向分类问题

接着我们转向另一个重要的机器学习问题：**分类**。本讲的重点是说明同样的线性拟合思路也可以用于分类问题，只需要一些小的调整。

<br />

### 2.3 输出类型的变化

在分类问题中，输出**不是连续值**，而是**离散的类别标签**。在黑板上表示为：
$$
y \in \{1,2,3\}.
$$
因此，任务是判断输入数据属于哪个类别。

<br />

### 2.4 什么是观测值

$x$ 代表观测值，可以是多种形式：
- 图像（课堂上常用图像作为例子）
- 温度数据
- 声音信号
- 或者是这些特征的组合

一个典型应用是自动驾驶：系统每秒采集多次图像，需要识别行人、车辆、自行车等目标，然后将这些信息传递给控制系统。这就构成了使用离散标签的分类问题。

<br />

---

## 第三部分：示例：霍比特人与精灵

![板书笔记](/images/docs/Lecture3_Linearclassification/3.png)

<br />

### 3.1 从训练样本中学习

我们仍然使用模型 $y = f(x)$，函数 $f$ 的参数通过带标签的训练样本来学习。这部分的工作原理与回归问题相同：收集数据，然后拟合函数。

<br />

### 3.2 兽人陷阱的故事

课堂引入了一个有趣的例子。假设我们是兽人，设置了一个陷阱。如果陷阱抓到霍比特人很好，因为我们喜欢吃他们；但如果抓到精灵就很危险，会带来麻烦。因此观测值 $x$ 可以是生物的**身高**。

我们假设霍比特人较矮，精灵较高。这就是我们用来区分它们的特征。

<br />

### 3.3 生成数据

为了进行模拟，我们假设中土世界的身高分布符合正态分布，就像现实世界中的生物特征。我们为霍比特人生成五个随机样本，为精灵也生成五个样本。霍比特人用一种颜色表示，精灵用另一种颜色。

在样本中，我们注意到有一个特别高的霍比特人和一个相对较矮的精灵。这说明两个类别并不完全分离，存在重叠区域。在实际问题中这是常见情况，意味着我们不能期望达到100%的分类准确率。

<br />

### 3.4 接受重叠现象

由于存在重叠，我们必须允许一定的错误率。无论建立什么系统，都会出现一些错误判断。
- 例如：在癌症检测中，并非所有癌症都能被检测出来，有时健康样本也会被误判为癌症。

因此我们必须预期分类中存在一定的错误率。

<br />

### 3.5 扩展到二维特征

到目前为止，我们只使用了一个特征：身高。但我们可以增加另一个特征，比如体重。假设：霍比特人可能又矮又重，而精灵则又高又瘦。

这样就得到了**二维数据**：
- $x_1 =$ 身高
- $x_2 =$ 体重

现在每个训练样本都是一个二维点 $(x_1, x_2)$，而 $y$ 是类别标签。对于训练数据，我们可以分配数值标签，例如 $y=0$ 代表霍比特人，$y=1$ 代表精灵。

<br />

### 3.6 训练与预测

在机器学习中，我们通常区分两个阶段：
1.  **训练阶段** — 从带标签的样本中学习模型参数
2.  **预测阶段** — 在新样本上测试模型，判断其类别

当新的数据点到来时，任务是：判断它是霍比特人还是精灵。

<br />

### 3.7 第一个分类思路

课堂上，黑板上展示了一个新样本。学生们思考如何进行分类。直觉的回答是：应该归类为霍比特人，因为它最靠近其他霍比特人的数据点。

这自然引出了第一种分类方法：基于与训练样本的接近程度进行分类。

<br />

---

## 第四部分：最近邻分类器

![板书笔记](/images/docs/Lecture3_Linearclassification/4.png)

<br />

### 4.1 方法名称与核心思想
我们从"接近程度"的直观概念发展出一个具体方法：**最近邻分类器**（1-NN，以及后来的 $k$-NN）。思路很简单：新样本应该与其最接近的训练样本属于同一类别。

<br />

### 4.2 训练过程（存储所有数据）
训练过程极其简单：**只需要存储所有训练样本**（特征值和对应的类别标签）。这个阶段不需要进行参数拟合。

<br />

### 4.3 预测过程（寻找最近邻并复制其标签）
当新样本到来时，我们进行以下计算：
1.  对于**所有**训练样本，计算与新样本的距离
   （任何合理的距离度量都可以使用，我们只需要比较相对远近）
2.  如果某个距离**小于当前最小距离**，就更新当前最小值并**选择**该样本
3.  **返回**具有**最小距离**的样本的类别标签

这就是整个算法。我们将在练习中用 Python 实现这个算法。

<br />

### 4.4 分类问题的简单基线
我们还建立了基线方法来判断分类器的效果：
-   **随机分类** — 从类别集合中随机输出一个标签
-   **多数类别** — 总是输出训练数据中出现最频繁的标签

第二种基线方法通常比随机分类更好，特别是当一个类别**占绝对优势**时（例如99%的样本都属于一个类别）。如果"多数类别"方法已经达到99%的准确率，那么我们的方法必须**超过**这个数值才有意义。

<br />

### 4.5 重点总结
-   最近邻是一个**极其简单**的算法，但**效果出奇地好**
-   应该始终与**简单基线**进行比较；这能告诉我们问题的难度和需要达到的性能水平

<br />

---

## 第五部分：k-NN 的考量 → 线性拟合

![板书笔记](/images/docs/Lecture3_Linearclassification/5.png)

<br />

### 5.1 k-NN 中的可调整参数
当我们开始研究可以在最近邻方法中调整哪些参数时，发现了许多可能性：

-   **距离度量**。我们可以选择不同的距离计算方法
  一维欧几里得距离表示为：
  $$
  d(x,x_i)=\sqrt{(x-x_i)^2},
  $$
  而**曼哈顿距离**（L1距离）表示为：
  $$
  d(x,x_i)=|x-x_i|.
  $$
  还有许多其他距离度量方法；我们应该选择最适合数据特征的方法

-   **邻居数量**。$k$ 可以取 $1,3,\dots$ 等值
  对于二分类问题，$k=2$ 可能出现平票情况（一个霍比特人，一个精灵），因此 $k=3$ 可以避免平票

-   **计算效率**。基本的 1-NN 需要遍历所有训练点，对于大型数据集可能很慢，但**有快速的 NN 算法**（通常是近似算法），通过对空间进行划分来加速搜索

**🤔 思考题：** 给出一个例子，其中 **1-NN** 和 **3-NN** 会给出**不同**的分类结果

<br />

### 5.2 能否通过拟合直线进行分类？
![板书笔记](/images/docs/Lecture3_Linearclassification/6.png)


我们尝试将分类问题转化为**直线拟合**问题。我们为类别分配数值标签：
$$
y=-1 \;\text{表示霍比特人}, \qquad y=+1 \;\text{表示精灵}.
$$
然后拟合一条直线：
$$
\hat y = a x + b
$$
到这些 $(x,y)$ 数据对上（与回归方法类似）
直线拟合完成后，我们使用简单的**分类规则**：
$$
\hat y < 0 \Rightarrow \text{霍比特人}, \qquad
\hat y > 0 \Rightarrow \text{精灵}.
$$
$\hat y=0$ 的点就是**决策边界**。

在课堂演示中，我们拟合了直线，然后测试了一些新样本；使用这个规则，所有样本都被正确分类


![板书笔记](/images/docs/Lecture3_Linearclassification/7.png)

<br />

### 5.3 如何衡量分类误差？
对于分类问题，误差是**二元的**：
-   如果 $\hat y$ 给出**正确**类别 ⇒ $\text{错误}=0$
-   如果 $\hat y$ 给出**错误**类别 ⇒ $\text{错误}=1$

我们不接受"可能"的输出结果
注意这里的不匹配：直线拟合最小化的是**平方误差**，而不是这种 0/1 误差。这种不匹配可能会带来问题——我们将在后面解决这个问题

<br />

---

## 第六部分：从直线拟合到阶跃函数，再到 S 型函数

![板书笔记](/images/docs/Lecture3_Linearclassification/8.png)

<br />

### 6.1 阶跃规则与决策边界

我们保留拟合的直线 $\hat y = a x + b$，并在 $\hat y=0$ 处定义**决策边界**
黑板上的阶跃规则使用 **$\pm1$** 标签：

$$
y =
\begin{cases}
+1, & x > x_d,\\[4pt]
-1, & x < x_d .
\end{cases}
$$

等价地，使用直线的符号：

$$
\hat y < 0 \Rightarrow \text{霍比特人}, \qquad
\hat y > 0 \Rightarrow \text{精灵}.
$$


![板书笔记](/images/docs/Lecture3_Linearclassification/9.png)

<br />

### 6.2 使用逻辑函数（Sigmoid）近似阶跃函数

由于硬阶跃函数是不连续的，我们使用它的平滑近似——逻辑函数（"logsig"）：

$$
\operatorname{logsig}(x)=\frac{1}{1+e^{-x}} .
$$

将其与直线组合：

$$
\hat y \;=\; \operatorname{logsig}(a x + b)
=\frac{1}{1+e^{-(a x + b)}} .
$$

这里 $a$ 控制过渡的陡峭程度，$b$ 控制阈值位置
我们将类别标签重新映射到 $[0,1]$ 区间（霍比特人 $=0$，精灵 $=1$）

<br />

### 6.3 （当前的）训练信号

使用这种平滑输出，我们仍然可以通过最小化目标值（在 $\{0,1\}$ 区间）和 $\hat y$ 之间的**均方误差**来拟合 $a$ 和 $b$

<br />

---

## 第七部分：S型函数输出的均方误差，梯度推导（步骤详细，清晰易懂）

![板书笔记](/images/docs/Lecture3_Linearclassification/10.png)

<br />

### 7.1 模型，目标函数，损失函数

模型（一维特征 \(x$）：

$$
z_i = a x_i + b
$$

$$
\hat y_i = \sigma(z_i) = \frac{1}{1+e^{-z_i}}
$$ ($\sigma$ 表示 S 型函数)

目标值：

$$
y_i \in \{0,1\}
$$

均方误差 (MSE)：

$$
\mathcal{L}_{\text{MSE}}=\frac{1}{N}\sum_{i=1}^{N}\bigl(y_i-\hat y_i\bigr)^2
$$

<br />

### 7.2 为什么 \(\partial z_i/\partial b = 1\) 且 \(\partial z_i/\partial a = x_i\)？

我们有一条直线：

$$
z_i = a x_i + b
$$

对 $b$ 求偏导数：

$$
\frac{\partial z_i}{\partial b} = 1
$$

对 $a$ 求偏导数：

$$
\frac{\partial z_i}{\partial a} = x_i
$$

所以改变 $b$ 会使直线上下平移（每单位 $b$ 移动 1 个单位）；改变 $a$ 会改变直线的斜率，变化幅度与 $x_i$ 成正比

<br />

### 7.3 S型函数的导数

定义：

$$
\sigma(z)=\frac{1}{1+e^{-z}}
$$ (其中 $e$ 是自然常数)

导数：

$$
\frac{d\sigma}{dz}=\sigma(z)\bigl(1-\sigma(z)\bigr)
$$

<br />

### 7.4 对均方误差求导（链式法则）

残差：

$$
e_i = y_i - \hat y_i
$$

用残差表示损失函数：

$$
\mathcal{L}_{\text{MSE}}=\frac{1}{N}\sum_{i=1}^{N} e_i^{\,2}
$$

外层平方项的导数：

$$
\frac{\partial}{\partial e_i}\bigl(e_i^{\,2}\bigr)=2e_i
$$

残差对输出的导数：

$$
\frac{\partial e_i}{\partial \hat y_i} = -1
$$

S型函数的链式求导（与直线组合）：

$$
\frac{\partial \hat y_i}{\partial a} = \frac{d\sigma}{dz}\Big|_{z=z_i}\cdot \frac{\partial z_i}{\partial a}
$$

$$
\frac{\partial \hat y_i}{\partial a} = \sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\cdot x_i
$$

$$
\frac{\partial \hat y_i}{\partial b} = \sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\cdot 1
$$

现在组合链式法则求 $a$ 的导数：

$$
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial a}
=\frac{1}{N}\sum_{i=1}^{N} 2e_i \cdot \frac{\partial e_i}{\partial \hat y_i}\cdot \frac{\partial \hat y_i}{\partial a}
$$

$$
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial a}
=\frac{1}{N}\sum_{i=1}^{N} 2e_i \cdot (-1)\cdot \sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\,x_i
$$

类似地，对于 $b$：

$$
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial b}
=\frac{1}{N}\sum_{i=1}^{N} 2e_i \cdot (-1)\cdot \sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\cdot 1
$$

将 $e_i=y_i-\hat y_i$ 代入，并将负号并入残差项（更简洁的形式）：

$$
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial a}
=\frac{2}{N}\sum_{i=1}^{N}\bigl(\hat y_i-y_i\bigr)\,\sigma(z_i)\bigl(1-\sigma(z_i)\bigr)\,x_i
$$

$$
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial b}
=\frac{2}{N}\sum_{i=1}^{N}\bigl(\hat y_i-y_i\bigr)\,\sigma(z_i)\bigl(1-\sigma(z_i)\bigr)
$$

<br />

### 7.5 各个因子的含义

-   残差项：
$$
\hat y_i - y_i
$$

-   S型函数斜率：
$$
\sigma(z_i)\bigl(1-\sigma(z_i)\bigr)
$$

-   输入尺度因子（仅对 $a$ 有）：
$$
x_i
$$

在 0.5 附近时，S型函数的斜率最大（学习信号最强）；接近 0 或 1 时斜率很小（处于饱和状态）

<br />

### 7.6 为什么没有解析解

参数 $a,b$ 位于求和符号**内部**的非线性 S型函数中。将两个梯度设为零无法得到简化的解析方程（不像线性回归那样）。因此，我们需要使用**迭代**方法：

学习率更新：

$$
a \leftarrow a - \eta\,\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial a}
$$

$$
b \leftarrow b - \eta\,\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial b}
$$ ($\eta$ 是学习率，控制每次更新的步长)

重复直到损失函数不再明显下降