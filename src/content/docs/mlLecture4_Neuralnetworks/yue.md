---
title: "機器學習 4：神經網絡"
subtitle: "DATA.ML.100 · Joni Kämäräinen · 2025年9月4日（四） TB104"
date: "2025-09-08"
lang: "yue"
excerpt: "由人腦啟發開始，講到線性閾值單元 (LTU)、Hebbian 學習、XOR 限制、非線性激活、反向傳播，再到現代框架（TensorFlow 同 PyTorch）。"
tags: ["Joni Kämäräinen", "機器學習", "神經網絡","TensorFlow", "PyTorch"]
draft: false
---
## 第1部分：由大腦到神經網絡

![板書](/images/docs/mlLecture4_Neuralnetworks/1.png)

### 1.1 點解要學神經網絡？
- 工程師遇到樽頸時，成日會向其他學科「借橋」（例如生物學、人文科學）。  
- 人腦係而家已知最聰明嘅「電腦」，所以成為機器學習嘅靈感來源。  

### 1.2 大腦同進化
- 腦容量受 **頭骨／產道** 限制，唔可以無限大。  
- 食物供應都會影響進化（例如島嶼族群會變細）。  
- 各物種神經元數量（大概）：
  - 海星：~500  
  - 水蛭：~10,000  
  - 果蠅：~100,000  
  - 蟑螂：~1,000,000  
  - 烏龜：~10,000,000  
  - 倉鼠：~100,000,000  
  - 貓頭鷹：~1,000,000,000  
  - 大猩猩：~30,000,000,000  
  - 人類：~100,000,000,000  
- 關鍵唔單止係神經元數量，仲係 **連接數**（人腦約 ~$10^{13}$ 個突觸）。  

### 1.3 生物神經元結構
- **樹突** = 輸入（$x_1, x_2, …$）  
- **突觸** = 權重（可以興奮／抑制）  
- **細胞體 (soma)** = 計算單元（似「線性模型 + 激活函數」）  
- **軸突** = 輸出（$y$）  

➡️ **人工神經元模型 = 輸入 × 權重 → 加總 → 激活 → 輸出**

### 1.4 補充
- 生物神經元用 **脈衝** 傳遞；人工神經元用連續數值。  
- 延伸方向：  
  - **脈衝式神經網絡 (SNNs)**  
  - **類腦晶片 (Neuromorphic chips)**

### 1.5 人工神經元等於線性模型
- 本質上就係一個 **線性模型**：輸入加權和再加偏置。  
- 同你之前學嘅 **線性回歸／分類** 一脈相承。  
- 呢啲基礎就係理解神經網絡嘅地基。
---

## 第2部分：線性模型同第一次神經網絡浪潮（1943）

![板書](/images/docs/mlLecture4_Neuralnetworks/2.png)

### 2.1 神經元作為線性模型
- 神經元可寫成 **輸入線性組合 + 偏置**：  
  $$
  y = \sum_i w_i x_i + w_0
  $$
- 呢套已經可以做 **線性回歸**。  
- 但做分類未必夠 → 需要加**閾值**或者**非線性激活**。  

### 2.2 邁向分類
- 二分類：得 0/1 兩類。  
- 做法：  
  - 先計加權和；  
  - 再用閾值（或者 sigmoid / logistic）決定輸出。  

### 2.3 歷史 — 第一次浪潮（1943）
- **McCulloch & Pitts (1943)：線性閾值單元（LTU）**  
- 定義：  
  $$
  y =
    \begin{cases}
      1 & \text{若 } w_1x_1 + w_2x_2 + w_0 \ge \theta \\
      0 & \text{若 } w_1x_1 + w_2x_2 + w_0 < \theta
    \end{cases}
  $$
- 通常用 $\theta = 0$。  
- 目的：模擬邏輯閘（數碼電腦嘅基礎）。  

### 2.4 例子：AND（與）閘
- 輸入：$x_1, x_2 \in \{0,1\}$  
- 期望：兩個都係 1 先輸出 1。  
- 權重設定：$w_1 = w_2 = \tfrac{2}{3},\ w_0 = -1$  

驗證：  
$$
\begin{aligned}
(0,0):\ & -1<0 \ \Rightarrow y=0 \\
(1,0)\ \text{或}\ (0,1):\ & \tfrac{2}{3}-1=-\tfrac{1}{3}<0 \ \Rightarrow y=0 \\
(1,1):\ & \tfrac{4}{3}-1=\tfrac{1}{3}>0 \ \Rightarrow y=1
\end{aligned}
$$  
✅ 可以實現 AND。  

### 2.5 例子：OR（或）閘
![板書](/images/docs/mlLecture4_Neuralnetworks/3.png)

- 輸入：$x_1, x_2 \in \{0,1\}$  
- 期望：有任一為 1 就輸出 1。  
- 簡單設定：$w_1 = w_2 = 1,\ w_0 = -1$  
  - (0,0)：$-1<0 \Rightarrow y=0$  
  - (1,0) 或 (0,1)：$1-1=0 \Rightarrow y=1$  
  - (1,1)：$2-1=1>0 \Rightarrow y=1$  
✅ 可行，但決策邊界較「貼線」。  

- 改良：$w_1 = w_2 = \tfrac{3}{2},\ w_0 = -1$ → **更大間隔**，冇咁敏感。  

### 2.6 小作業：NAND 閘
> 用 LTU 設計一個 **NAND**。  
> 提示：NAND = NOT(AND)。由 AND 權重出發再調整。  

### 2.7 重點
- 一個簡單 LTU，揀啱權重就做到邏輯功能。  
- 歷史上首次證明：神經元可以作為計算嘅基本單元。

---

## 第3部分：第二浪潮 (1949) — Hebbian 学习

![板書](/images/docs/mlLecture4_Neuralnetworks/4.png)

### 3.1 Hebbian 学习规则
- 當時提出用嚟更新 LTU 權重嘅方法。  
- 更新公式：  
  $$
  w_i^{t+1} = w_i^t + \eta (y - \hat{y}) x
  $$
- 當中：  
  - $y$ = 真實輸出（標籤）  
  - $\hat{y}$ = 預測輸出  
  - 誤差 = $y - \hat{y}$  
  - $\eta$ = 學習率（例如 0.1）  

### 3.2 學習過程
- 如果預測過高 → 權重減少。  
- 如果預測過低 → 權重增加。  
- 重複多次之後，權重會收斂，令 $y \approx \hat{y}$。  

### 3.3 學習率調整
- 學習率太大 → 出現震盪（誤差收唔到）。  
- 解決方法：**慢慢減低學習率**，咁先可以保證收斂。  

---

## 第4部分：單個神經元嘅限制 — XOR 同第二次冬天

![板書](/images/docs/mlLecture4_Neuralnetworks/5.png)

### 4.1 線性可分性
- 感知機只可以解決 **線性可分** 嘅問題（用一條直線／超平面就可以分開）。  
- 例子：AND、OR 可以做到。  

### 4.2 XOR 問題
- XOR 真值表：  
  - (0,0) → 0  
  - (1,0) → 1  
  - (0,1) → 1  
  - (1,1) → 0  
- XOR **唔可以線性可分** → 單個感知機解唔到。  

### 4.3 第二次 AI 冬天
- 1969 年：Minsky & Papert 證明咗感知機嘅限制（特別係 XOR）。  
- 學界開始失望，資金減少 → **第二次 AI 冬天**。  

### 4.4 多層想法
- 生物大腦係靠 **好多神經元一齊合作**。  
- 如果加多幾層（隱層），XOR 係可以解到嘅。  
- 1970–80年代嘅 **反向傳播演算法** 令到訓練多層網絡變得可行。  

---

## 第5部分：由線性到非線性 — Logistic、損失函數同梯度下降

![板書](/images/docs/mlLecture4_Neuralnetworks/6.png)

### 5.1 激活函數
- 想突破線性回歸，就要用非線性嘅激活：  
  - **Sigmoid (logistic)：**  
    $$\sigma(z) = \frac{1}{1+e^{-z}} \quad \in (0,1)$$  
  - **Tanh：**  
    $$\tanh(z) \in (-1,1)$$  
  - 其他：線性、ReLU 等等。  

### 5.2 損失函數
- 回歸：均方誤差 (MSE)  
  $$L = \frac{1}{N} \sum (y - \hat{y})^2$$  
- 分類：交叉熵（之後引入）。  

### 5.3 梯度下降
- 計損失對權重嘅梯度：  
  $$\nabla_w L = \left( \frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_2}, … \right)$$  
- 更新公式：  
  $$
  w^{(t+1)} = w^{(t)} - \eta \, \nabla_w L
  $$
- $\eta$ = 學習率 — 太大 → 發散；太細 → 學得好慢。  
- **學習率調整** 可以幫助收斂。  

### 5.4 萬能近似定理
- 只要有 **至少一個隱層** 同非線性激活，神經網絡就可以近似 **任意函數**（Cybenko 1989）。  

### 5.5 多層感知機同反向傳播
![板書](/images/docs/mlLecture4_Neuralnetworks/7.png)

- 單個 LTU 解唔到 XOR。  
- 解決方法：用 **多層感知機 (MLP)**，即係多幾層神經元。  
  - 例如：兩個隱層神經元 ($y_1, y_2$) 再去到輸出神經元。  
  - 咁樣就可以有非線性嘅決策邊界。  

➡️ **反向傳播演算法**  
- 計輸出誤差（預測同目標之間嘅差）。  
- 用 **鏈式法則** 將誤差傳返去前一層。  
- 所有權重（輸入→隱層，隱層→輸出）都會更新。  

👉 呢個突破令到訓練深度網絡成為可能，解決咗 XOR 嘅限制。  

### 5.6 多分類問題
![板書](/images/docs/mlLecture4_Neuralnetworks/8.png)

- 之前淨係講過 **二分類** (0/1)。  
- 如果係 **多分類 (N 類)**：  
  - 輸出層有 **N 個神經元**，每個對應一個類。  
  - 例如：3 類 → 輸出 = $(y_1, y_2, y_3)$。  
- **標籤** 用 **one-hot encoding**：  
  - 類別1 → (1,0,0)  
  - 類別2 → (0,1,0)  
  - 類別3 → (0,0,1)  
- 訓練時：  
  - 前向傳播：計所有類嘅輸出。  
  - 損失：交叉熵 + softmax。  
  - 反向傳播：更新所有權重。  

---

## 第6部分：實踐 — TensorFlow 同 PyTorch

![板書](/images/docs/mlLecture4_Neuralnetworks/9.png)

### 6.1 TensorFlow 同 PyTorch
- 兩個都係 **主流深度學習框架**。  

- **TensorFlow (Google)**  
  - 本課堂揀咗佢 *「為咗多樣性」*。  
  - `Sequential` API 好簡單：逐層加就得。  
  - 啱做 **入門示範**（例如線性回歸）。  
  - 安裝可能會報 *「少少錯誤」*，但唔影響使用。  

- **PyTorch (Meta/Facebook)**  
  - *「我覺得 PyTorch 已經比 TensorFlow 更受歡迎。」*  
  - 更加靈活，更接近 Python，本身適合研究。  
  - *「PyTorch 教學上更有幫助，但都比較難。」*  
  - 深度學習課會用。  

- **總結**  
  - **TensorFlow** → 易上手，適合初學者，課堂示範。  
  - **PyTorch** → 研究界更流行，靈活但難啲。  

- 講課原話：  
  > *「冇人會手動做 backprop，除非係學習時練習。真實工作用 TensorFlow 或者 PyTorch。」*  

---

### 6.2 單神經元回歸
- 模型：`Dense(units=1, activation="linear")`  
- 優化器：SGD（隨機梯度下降）。  
- 損失：MSE。  
- 訓練 (`fit`) 顯示誤差會慢慢減少。  
- ✅ 學到咗點樣用神經網絡擬合一條直線。  

---

### 6.3 多層感知機 (MLP) 回歸
- 任務：近似一個 **正弦波**。  
- 輸入：時間 $t$。  
- 隱層：50 個神經元，**sigmoid 激活**。  
- 輸出：$y \approx \sin(t)$。  
- 展示 **萬能近似定理**：  
  - 只要有足夠神經元，一個隱層已經可以近似任何函數。  
- 訓練時要調校 **學習率** 同 **epoch** 去減少震盪。  

---

### 6.4 分類例子
- 任務：分類「**哈比人 vs. 精靈**」。  
- 輸出層：2 個神經元（二分類）。  
- 標籤用 **one-hot encoding**：  
  - 哈比人 → (1,0)  
  - 精靈 → (0,1)  
- 損失：交叉熵。  
- 準確率會隨住 epoch 提升（例如 86% → 92%）。  

---

### 6.5 功課
- 用 **TensorFlow** 訓練你自己嘅神經網絡。  
- 可以試：  
  - 唔同學習率。  
  - 更多 epoch。  
  - 加多啲隱層神經元／層數。  
- 挑戰：超越 **Hebbian 學習** 同課堂示範嘅準確率。  

---

## 總結

1. **由大腦到神經網絡**  
   - 神經網絡受生物神經元啟發：輸入、權重、加總、激活。  
   - 大腦展示咗龐大連接 (~$10^{13}$ 突觸) 嘅威力。  

2. **第一次浪潮 (1943)：LTU**  
   - McCulloch & Pitts 證明咗單個神經元可以做邏輯功能（AND、OR、NAND）。  
   - 但係冇學習機制。  

3. **第二次浪潮 (1949)：Hebbian 學習**  
   - 提出根據誤差去調整權重。  
   - 係早期學習方法，但對學習率好敏感。  

4. **單神經元限制 (XOR)**  
   - 感知機只可以解線性可分嘅問題。  
   - XOR 揭示咗限制 → 導致第二次 AI 冬天。  

5. **非線性激活同反向傳播**  
   - logistic、tanh 等激活函數令網絡有非線性邊界。  
   - 反向傳播 (1970–80年代) 令訓練多層感知機成為可能。  
   - 萬能近似定理 (1989)：一個隱層就可以近似任意函數。  

6. **現代實踐：TensorFlow 同 PyTorch**  
   - TensorFlow：簡單、啱初學者、課堂用。  
   - PyTorch：研究界更流行，更靈活，但難啲。  
   - 框架令回歸、分類、多層網絡嘅訓練實際可行。  
