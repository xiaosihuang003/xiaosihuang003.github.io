---
title: "Maskininl√§rning 4: Neurala n√§tverk"
subtitle: "DATA.ML.100 ¬∑ Joni K√§m√§r√§inen ¬∑ Tors 4.9.2025 TB104 üòä"
date: "2025-09-08"
lang: "sv"
excerpt: "Grunderna i neurala n√§tverk: fr√•n biologisk inspiration till LTU, Hebbiansk inl√§rning, XOR-begr√§nsningar, icke-linj√§ra aktiveringsfunktioner, backpropagation och moderna ramverk (TensorFlow & PyTorch)."
tags: ["Joni K√§m√§r√§inen", "maskininl√§rning", "neurala-n√§tverk", "TensorFlow", "PyTorch"]
draft: false
---

## Del 1 : Fr√•n hj√§rnor till neurala n√§tverk

![Board notes](/images/docs/mlLecture4_Neuralnetworks/1.png)

### 1.1 Varf√∂r studera neurala n√§tverk?
- N√§r ingenj√∂rer st√∂ter p√• flaskhalsar l√•nar de ofta id√©er fr√•n andra vetenskaper (biologi, humaniora, etc.).  
- Den m√§nskliga hj√§rnan √§r den mest intelligenta ‚Äùdatorn‚Äù vi k√§nner till och fungerar d√§rf√∂r som inspiration f√∂r maskininl√§rning.  

### 1.2 Hj√§rnan och evolutionen
- Hj√§rnans storlek √§r begr√§nsad av **skalle/f√∂delsekanal** ‚Äî den kan inte v√§xa obegr√§nsat.  
- Tillg√•ng till mat p√•verkar ocks√• evolutionen (t.ex. √∂populationer som blir mindre).  
- Antal neuroner i olika arter:
  - Sj√∂stj√§rna: ~500  
  - Iglar: ~10,000  
  - Bananfluga: ~100,000  
  - Kackerlacka: ~1,000,000  
  - Sk√∂ldpadda: ~10,000,000  
  - Hamster: ~100,000,000  
  - Uggla: ~1,000,000,000  
  - Gorilla: ~30,000,000,000  
  - M√§nniska: ~100,000,000,000  
- Viktig faktor √§r inte bara antalet neuroner, utan **antalet kopplingar** (~$10^{13}$ synapser i den m√§nskliga hj√§rnan).  

### 1.3 Biologisk neurons struktur
- **Dendriter** = indata ($x_1, x_2, ‚Ä¶$)  
- **Synapser** = vikter (kan excitera eller h√§mma)  
- **Cellkropp (soma)** = ber√§kningsenhet (som en linj√§r modell + aktiveringsfunktion)  
- **Axon** = utdata ($y$)  

‚û°Ô∏è **Artificiell neuronmodell = Indata √ó Vikter ‚Üí Summering ‚Üí Aktiveringsfunktion ‚Üí Utdata**

### 1.4 Anteckningar
- Biologiska neuroner skickar signaler som **spikar**, medan artificiella neuroner anv√§nder kontinuerliga v√§rden.  
- Detta leder till framv√§xande omr√•den som:  
  - **Spikande neurala n√§tverk (SNNs)**  
  - **Neuromorfa chip**

### 1.5 Artificiell neuron som linj√§r modell
- I grunden kan en artificiell neuron beskrivas som en **linj√§r modell**: en viktad summa av indata plus en bias.  
- Detta kopplar direkt till vad vi studerade i **linj√§r regression** och **klassificering**.  
- Dessa tidigare √§mnen √§r grunden f√∂r att f√∂rst√• neurala n√§tverk.  

---

## Del 2 : Linj√§r modell och den f√∂rsta v√•gen av neurala n√§tverk (1943)

![Board notes](/images/docs/mlLecture4_Neuralnetworks/2.png)

### 2.1 Linj√§r modell som en neuron
- Neuronen kan modelleras som en **linj√§r kombination av indata** plus en bias:  
  $$
  y = \sum_i w_i x_i + w_0
  $$
- Detta system kan redan utf√∂ra **linj√§r regression**.  
- F√∂r klassificering √§r dock linj√§r regression inte tillr√§cklig ‚Üí vi beh√∂ver tr√∂skel eller en icke-linj√§r aktivering.  

### 2.2 Mot klassificering
- Bin√§r klassificering: endast tv√• klasser (0/1).  
- L√∂sning: applicera en **tr√∂skelfunktion** (eller senare sigmoid/logistisk).  
- I kod:  
  - F√∂rst ber√§kna den viktade summan.  
  - Sedan applicera tr√∂skel (eller sigmoid) f√∂r att best√§mma utdata.  

### 2.3 Historisk not ‚Äî V√•gen 1 (1943)
- **McCulloch & Pitts (1943): Linj√§r tr√∂skelenhet (LTU)**  
- Definition:  
  $$
  y = 
    \begin{cases}
      1 & \text{om } w_1x_1 + w_2x_2 + w_0 \geq \theta \\
      0 & \text{om } w_1x_1 + w_2x_2 + w_0 < \theta
    \end{cases}
  $$
- Typiskt tr√∂skelv√§rde $\theta = 0$.  
- Motivation: simulera logiska grindar (grunden f√∂r digitala datorer).  

### 2.4 Exempel: OCH-grind
- Indata: $x_1, x_2 \in \{0,1\}$  
- √ñnskad utdata: $y=1$ endast n√§r b√•da indata √§r 1.  
- V√§lj vikter: $w_1 = w_2 = \tfrac{2}{3},\ w_0 = -1$  

---

## Del 3 : V√•gen 2 (1949) ‚Äî Hebbiansk inl√§rning

![Board notes](/images/docs/mlLecture4_Neuralnetworks/4.png)

### 3.1 Hebbiansk inl√§rningsregel
- F√∂reslogs som en uppdateringsmekanism f√∂r vikter i LTU.  
- Uppdateringsregel:  
  $$
  w_i^{t+1} = w_i^t + \eta (y - \hat{y}) x
  $$
- D√§r:  
  - $y$ = sant utdata (ground truth)  
  - $\hat{y}$ = uppskattat utdata  
  - fel = $y - \hat{y}$  
  - $\eta$ = inl√§rningshastighet (t.ex. 0.1)  

### 3.2 Inl√§rningsdynamik
- Om uppskattningen √§r f√∂r h√∂g ‚Üí vikterna minskar.  
- Om uppskattningen √§r f√∂r l√•g ‚Üí vikterna √∂kar.  
- Med tiden konvergerar vikterna till en punkt d√§r $y \approx \hat{y}$.  

### 3.3 Justering av inl√§rningshastighet
- Stor $\eta$ kan orsaka oscillation (felet stabiliseras inte).  
- L√∂sning: **minska inl√§rningshastigheten gradvis** f√∂r att s√§kerst√§lla konvergens.  

---

## Del 4 : Begr√§nsningar hos en enda neuron ‚Äî XOR och den andra vintern

![Board notes](/images/docs/mlLecture4_Neuralnetworks/5.png)

### 4.1 Linj√§r separabilitet
- En perceptron kan bara l√∂sa problem d√§r data √§r **linj√§rt separerbar** (en linje/hyperplan delar klasserna).  
- Exempel: OCH, ELLER fungerar.  

### 4.2 XOR-problemet
- XOR sannolikhetstabell:  
  - (0,0) ‚Üí 0  
  - (1,0) ‚Üí 1  
  - (0,1) ‚Üí 1  
  - (1,1) ‚Üí 0  
- Ej linj√§rt separerbar ‚Äî kan inte l√∂sas med en enda perceptron.  

### 4.3 Den andra AI-vintern
- 1969: Minsky & Papert visade begr√§nsningarna hos perceptrons (speciellt XOR).  
- Ledde till skepsis och minskad finansiering ‚Üí ‚Äùandra vintern.‚Äù  

### 4.4 Id√©n om flerskikt
- Biologiska hj√§rnor har **m√•nga neuroner som arbetar tillsammans**.  
- Om vi kombinerar flera neuroner (ett dolt skikt) kan XOR representeras.  
- Backpropagation-algoritmen (1970‚Äì80-tal) m√∂jliggjorde effektiv tr√§ning av s√•dana n√§tverk.  

---

## Del 5 : Fr√•n linj√§r till icke-linj√§r ‚Äî Logistisk funktion, f√∂rlust & gradientnedstigning

![Board notes](/images/docs/mlLecture4_Neuralnetworks/6.png)

### 5.1 Aktiveringsfunktioner
- F√∂r att g√• bortom linj√§r regression anv√§nder vi icke-linj√§ra aktiveringar:  
  - **Sigmoid (logistisk):**  
    $$\sigma(z) = \frac{1}{1+e^{-z}} \quad \in (0,1)$$  
  - **Tanh:**  
    $$\tanh(z) \in (-1,1)$$  
  - Linj√§r (identitet), ReLU, etc. (senare utvecklingar).  

### 5.2 F√∂rlustfunktioner
- Regression: Medelkvadratfel (MSE)  
  $$L = \frac{1}{N} \sum (y - \hat{y})^2$$  
- Klassificering: Korsentropi (inf√∂rdes senare).  

### 5.3 Gradientnedstigning
- Ber√§kna gradienten av f√∂rlusten med avseende p√• vikterna:  
  $$\nabla_w L = \left( \frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_2}, ‚Ä¶ \right)$$  
- Uppdateringsregel:  
  $$
  w^{(t+1)} = w^{(t)} - \eta \, \nabla_w L
  $$
- $\eta$: inl√§rningshastighet ‚Äî f√∂r stor ‚Üí divergens, f√∂r liten ‚Üí l√•ngsam tr√§ning.  
- **Schemal√§ggning av inl√§rningshastighet** hj√§lper konvergens.  

### 5.4 Universella approximationsteoremet
- Med minst **ett dolt skikt** och icke-linj√§r aktivering kan ett neuralt n√§tverk approximera *vilken funktion som helst* (Cybenko 1989).  

### 5.5 Flerskiktsperceptron & backpropagation
![Board notes](/images/docs/mlLecture4_Neuralnetworks/7.png)

- En **enda LTU** kan inte l√∂sa problem som XOR.  
- L√∂sning: kombinera flera neuroner till en **flerskiktsperceptron (MLP)**.  
  - Exempel: tv√• dolda neuroner ($y_1, y_2$) som matar in i en utdata-neuron.  
  - Denna struktur m√∂jligg√∂r icke-linj√§ra beslutsgr√§nser.  

‚û°Ô∏è **Backpropagation-algoritmen**  
- Ber√§kna utdata-felet (skillnaden mellan prediktion och m√•l).  
- Propagera felet bak√•t genom n√§tverket med hj√§lp av **kedjeregeln**.  
- Uppdatera alla vikter (indata ‚Üí dolt, dolt ‚Üí utdata) via gradientnedstigning.  

### 5.6 Flerklassklassificering

![Board notes](/images/docs/mlLecture4_Neuralnetworks/8.png)

- Hittills har vi bara beaktat **bin√§r klassificering** (0/1).  
- F√∂r **flerklassproblem (N klasser)**:  
  - Utdatalagret har **N neuroner**, en f√∂r varje klass.  
  - Exempel: 3 klasser ‚Üí utdata = $(y_1, y_2, y_3)$.  
- **Ground truth-etiketter** anv√§nder **one-hot-kodning**:  
  - Klass 1 ‚Üí (1, 0, 0)  
  - Klass 2 ‚Üí (0, 1, 0)  
  - Klass 3 ‚Üí (0, 0, 1)  
- Under tr√§ning:  
  - Fram√•tpass: ber√§kna utdata f√∂r alla klasser.  
  - F√∂rlust: ofta **korsentropi** med softmax i utdata.  
  - Bak√•tpass: propagera felet genom alla vikter med backpropagation.  

‚û°Ô∏è Detta generaliserar neurala n√§tverk f√∂r att hantera **vilket antal klasser som helst**.  

---

## Del 6 : Praktik med ramverk ‚Äî TensorFlow & PyTorch

![Board notes](/images/docs/mlLecture4_Neuralnetworks/9.png)

### 6.1 TensorFlow vs. PyTorch
- B√•da √§r de **mest anv√§nda djupinl√§rningsramverken**.  

- **TensorFlow (Google)**  
  - Valdes i denna kurs *‚Äúf√∂r m√•ngfaldens skull.‚Äù*  
  - `Sequential` API √§r mycket enkelt: du kan l√§gga till lager ett efter ett.  
  - Bra f√∂r en **f√∂rsta demo av neuralt n√§tverk** (t.ex. linj√§r regression).  
  - Installation kan ge *‚Äún√•gra sm√• fel‚Äù* men fungerar √§nd√•.  

- **PyTorch (Meta/Facebook)**  
  - *‚ÄúPyTorch √§r nog mer popul√§rt √§n TensorFlow.‚Äù*  
  - Mer flexibelt och n√§rmare r√• Python, b√§ttre f√∂r komplexa modeller.  
  - *‚ÄúPyTorch √§r mer l√§rorikt, men ocks√• sv√•rare.‚Äù*  
  - Kommer att anv√§ndas i **Deep Learning-kursen** senare.  

- **Sammanfattning**  
  - **TensorFlow** ‚Üí enklare, nyb√∂rjarv√§nligt, bra f√∂r demos.  
  - **PyTorch** ‚Üí mer popul√§rt i forskning, flexibelt, mer l√§rorikt men sv√•rare.  

- Citat fr√•n f√∂rel√§sningen:  
  > *‚ÄúIngen g√∂r backpropagation manuellt, utom om du vill l√§ra dig. F√∂r riktigt arbete anv√§nder vi TensorFlow eller PyTorch.‚Äù*  

---

### 6.2 Enkel regression med en neuron
- Modell: `Dense(units=1, activation="linear")`  
- Optimerare: SGD (stokastisk gradientnedstigning).  
- F√∂rlust: MSE (medelkvadratfel).  
- Tr√§ning (`fit`) visar att felet minskar √∂ver epoker.  
- ‚úÖ L√§r sig en regressionslinje (som linj√§r regression).  

---

### 6.3 Flerskiktsperceptron (MLP) regression
- Uppgift: approximera en **sinusv√•g**.  
- Indata: $t$ (tid).  
- Dolt lager: 50 neuroner, **sigmoidaktivering**.  
- Utdata: $y \approx \sin(t)$.  
- Demonstrerar **det universella approximationsteoremet**.  
- Tr√§ning kr√§ver justering av **inl√§rningshastighet** och **epoker** f√∂r att minska oscillation och fel.  

---

### 6.4 Klassificeringsexempel
- Uppgift: klassificera ‚Äú**Hobbits vs. Alver**.‚Äù  
- Utdatalager: 2 neuroner (bin√§r klassificering).  
- Etiketter kodas med **one-hot**:  
  - Hobbit ‚Üí (1,0)  
  - Alv ‚Üí (0,1)  
- F√∂rlust: korsentropi.  
- Noggrannheten f√∂rb√§ttras med fler epoker (t.ex. 86% ‚Üí 92%).  

---

### 6.5 Heml√§xa
- Prova att tr√§na egna neurala n√§tverk i **TensorFlow**.  
- Experimentera med:  
  - Olika inl√§rningshastigheter.  
  - Fler epoker.  
  - Fler dolda neuroner/lager.  
- Utmaning: sl√• noggrannheten fr√•n **Hebbiansk inl√§rning** och l√§rarens demos.  

---

## Sammanfattning

1. **Fr√•n hj√§rnor till neurala n√§tverk**  
   - Neurala n√§tverk √§r inspirerade av biologiska neuroner: indata, vikter (synapser), summering och aktivering.  
   - Hj√§rnan visar kraften hos massiva kopplingar (~$10^{13}$ synapser).  

2. **V√•gen 1 (1943): Linj√§ra tr√∂skelenheter**  
   - McCulloch & Pitts visade att enkla neuroner kan implementera logiska funktioner (OCH, ELLER, NAND).  
   - Men de hade ingen inl√§rningsmekanism.  

3. **V√•gen 2 (1949): Hebbiansk inl√§rning**  
   - Introducerade id√©n om att justera vikter baserat p√• fel.  
   - Tidig form av inl√§rning, men k√§nslig f√∂r inl√§rningshastighet och konvergens.  

4. **Begr√§nsningar hos en neuron (XOR)**  
   - En perceptron kan bara l√∂sa linj√§rt separerbara problem.  
   - XOR avsl√∂jade denna begr√§nsning ‚Üí ledde till skepsis och andra AI-vintern.  

5. **Icke-linj√§ra aktiveringar & backpropagation**  
   - Sigmoid, tanh och andra aktiveringar m√∂jligg√∂r icke-linj√§ra beslutsgr√§nser.  
   - Backpropagation (1970‚Äì80-tal) gjorde det m√∂jligt att tr√§na flerskiktsn√§t.  
   - Universella approximationsteoremet (1989): ett dolt skikt r√§cker f√∂r att approximera vilken funktion som helst.  

6. **Modern praktik: TensorFlow & PyTorch**  
   - TensorFlow: nyb√∂rjarv√§nligt, bra f√∂r demos, anv√§nds i denna kurs.  
   - PyTorch: flexibelt, mer popul√§rt i forskning, anv√§nds i deep learning-kursen.  
   - Ramverken g√∂r det praktiskt att tr√§na regression, klassificering och flerskiktsn√§t i skala.  




