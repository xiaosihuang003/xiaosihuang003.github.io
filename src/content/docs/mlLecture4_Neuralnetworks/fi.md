---
title: "ML_4: Neuroverkot"
subtitle: "DATA.ML.100 ¬∑ Joni K√§m√§r√§inen ¬∑ To 4.9.2025 TB104 üòä"
date: "2025-09-08"
lang: "fi"
excerpt: "Neuroverkkojen perusteet: biologisesta inspiraatiosta LTU:ihin, Hebbin oppimiseen, XOR-rajoituksiin, ep√§lineaarisiin aktivointeihin, takaisinlevitykseen ja nykyaikaisiin kehyksiin (TensorFlow & PyTorch)."
tags: ["Joni K√§m√§r√§inen", "koneoppiminen", "neuroverkot", "TensorFlow", "PyTorch"]
draft: false
---

## Osa 1 : Aivoista neuroverkkoihin

![Board notes](/images/docs/mlLecture4_Neuralnetworks/1.png)

### 1.1 Miksi tutkia neuroverkkoja?
- Kun insin√∂√∂rit kohtaavat pullonkauloja, he lainaavat usein ideoita muista tieteist√§ (biologia, ihmistieteet jne.).  
- Ihmisen aivot ovat √§lykk√§in "tietokone", jonka tunnemme, ja ne toimivat koneoppimisen inspiraationa.  

### 1.2 Aivot ja evoluutio
- Aivojen koko on rajoitettu **kallon / synnytyskanavan** vuoksi ‚Äì ne eiv√§t voi kasvaa loputtomiin.  
- My√∂s ravinnon saatavuus vaikuttaa evoluutioon (esim. saarten populaatiot pienenev√§t).  
- Hermosolujen m√§√§r√§ eri lajeissa:
  - Merit√§hdell√§: ~500  
  - Iilimadolla: ~10,000  
  - Banaanik√§rp√§sell√§: ~100,000  
  - Torakalla: ~1,000,000  
  - Kilpikonnalla: ~10,000,000  
  - Hamsterilla: ~100,000,000  
  - P√∂ll√∂ll√§: ~1,000,000,000  
  - Gorillalla: ~30,000,000,000  
  - Ihmisell√§: ~100,000,000,000  
- T√§rkeint√§ ei ole vain hermosolujen m√§√§r√§, vaan **yhteyksien m√§√§r√§** (~$10^{13}$ synapsia ihmisaivoissa).  

### 1.3 Biologisen hermosolun rakenne
- **Dendriitit** = sy√∂tteet ($x_1, x_2, ‚Ä¶$)  
- **Synapsit** = painot (voivat kiihdytt√§√§ tai est√§√§)  
- **Solukeskus (soma)** = laskentayksikk√∂ (kuten lineaarinen malli + aktivointifunktio)  
- **Aksoni** = l√§ht√∂ ($y$)  

‚û°Ô∏è **Keinotekoinen neuroni = Sy√∂tteet √ó Painot ‚Üí Summataan ‚Üí Aktivointifunktio ‚Üí Ulostulo**

### 1.4 Huomioita
- Biologiset neuronit v√§litt√§v√§t signaaleja **piikkein√§**, kun taas keinotekoiset neuronit k√§ytt√§v√§t jatkuvia arvoja.  
- T√§m√§ johtaa uusiin tutkimusalueisiin kuten:  
  - **Spiking Neural Networks (SNNs)**  
  - **Neuromorfiset sirut**

### 1.5 Keinotekoinen neuroni lineaarisena mallina
- Keinotekoinen neuroni voidaan kuvata **lineaariseksi malliksi**: painotettujen sy√∂tteiden summa + bias.  
- T√§m√§ liittyy suoraan siihen, mit√§ olemme opiskelleet **lineaarisessa regressiossa** ja **luokittelussa**.  
- N√§m√§ aiheet toimivat perustana neuroverkkojen ymm√§rt√§miselle.  

---

## Osa 2 : Lineaarinen malli ja ensimm√§inen neuroverkkoaalto (1943)

![Board notes](/images/docs/mlLecture4_Neuralnetworks/2.png)

### 2.1 Neuroni lineaarisena mallina
- Neuroni voidaan mallintaa **sy√∂tteiden lineaarisena yhdistelm√§n√§** + bias:  
  $$
  y = \sum_i w_i x_i + w_0
  $$
- T√§m√§ j√§rjestelm√§ voi jo suorittaa **lineaarisen regression**.  
- Luokitteluun t√§m√§ ei kuitenkaan riit√§ ‚Üí tarvitaan kynnysfunktio tai ep√§lineaarinen aktivointi.  

### 2.2 Kohti luokittelua
- Bin√§√§riluokittelu: vain kaksi luokkaa (0/1).  
- Ratkaisu: k√§yt√§ **kynnysfunktiota** (tai my√∂hemmin sigmoid/logistista funktiota).  
- Koodissa:  
  - Lasketaan ensin painotettu summa.  
  - Sitten sovelletaan kynnyst√§ (tai sigmoidia) p√§√§t√∂ksen tekemiseksi.  

### 2.3 Historiallinen huomio ‚Äî Aalto 1 (1943)
- **McCulloch & Pitts (1943): Lineaarinen kynnysyksikk√∂ (LTU)**  
- M√§√§ritelm√§:  
  $$
  y = 
    \begin{cases}
      1 & \text{jos } w_1x_1 + w_2x_2 + w_0 \geq \theta \\
      0 & \text{jos } w_1x_1 + w_2x_2 + w_0 < \theta
    \end{cases}
  $$
- Tyypillisesti $\theta = 0$.  
- Motivaatio: simuloida logiikkaporteja (digitaalisten tietokoneiden perusta).  

### 2.4 Esimerkki: AND-portti
- Sy√∂tteet: $x_1, x_2 \in \{0,1\}$  
- Haluttu tulos: $y=1$ vain kun molemmat sy√∂tteet ovat 1.  
- Painot: $w_1 = w_2 = \tfrac{2}{3},\ w_0 = -1$  

Tarkistus:  
$$
\begin{aligned}
(0,0):\ & -1<0 \ \Rightarrow y=0 \\
(1,0)\ \text{tai}\ (0,1):\ & \tfrac{2}{3}-1=-\tfrac{1}{3}<0 \ \Rightarrow y=0 \\
(1,1):\ & \tfrac{4}{3}-1=\tfrac{1}{3}>0 \ \Rightarrow y=1
\end{aligned}
$$  

‚úÖ Toimii AND-porttina.  

### 2.5 Esimerkki: OR-portti
![Board notes](/images/docs/mlLecture4_Neuralnetworks/3.png)

- Sy√∂tteet: $x_1, x_2 \in \{0,1\}$  
- Haluttu tulos: $y=1$, jos jompikumpi sy√∂te on 1.  
- Painot (perusversio): $w_1 = w_2 = 1,\ w_0 = -1$  
  - (0,0): $-1<0 \Rightarrow y=0$  
  - (1,0) tai (0,1): $1-1=0 \Rightarrow y=1$  
  - (1,1): $2-1=1>0 \Rightarrow y=1$  

‚úÖ Toimii OR-porttina, mutta raja on herkk√§.  

Parannus: $w_1 = w_2 = \tfrac{3}{2},\ w_0 = -1$ antaa paremman marginaalin.  

### 2.6 Kotiteht√§v√§: NAND-portti
> Suunnittele **NAND-portti** LTU-mallilla.  
> - Vihje: NAND = NOT(AND).  
> - Aloita AND-portin painoista ja s√§√§d√§ niit√§.  

### 2.7 Yhteenveto
- Yksinkertainen LTU sopivilla painoilla voi toteuttaa loogisia funktioita.  
- T√§m√§ historiallinen askel osoitti, ett√§ hermoyksik√∂t voisivat teoriassa muodostaa laskennan perustan.  

---

## Osa 3 : Aalto 2 (1949) ‚Äî Hebbin oppiminen

![Board notes](/images/docs/mlLecture4_Neuralnetworks/4.png)

### 3.1 Hebbin oppimiss√§√§nt√∂
- Ehdotettiin painojen p√§ivitysmekanismiksi LTU:ssa.  
- P√§ivityss√§√§nt√∂:  
  $$
  w_i^{t+1} = w_i^t + \eta (y - \hat{y}) x
  $$
- Miss√§:  
  - $y$ = todellinen ulostulo (ground truth)  
  - $\hat{y}$ = arvioitu ulostulo  
  - virhe = $y - \hat{y}$  
  - $\eta$ = oppimisnopeus (esim. 0.1)  

### 3.2 Oppimisdynamiikka
- Jos arvio on liian korkea ‚Üí painot pienenev√§t.  
- Jos arvio on liian matala ‚Üí painot kasvavat.  
- Ajan my√∂t√§ painot konvergoivat pisteeseen, jossa $y \approx \hat{y}$.  

### 3.3 Oppimisnopeuden s√§√§t√∂
- Liian suuri $\eta$ voi aiheuttaa oskillaatiota (virhe ei asetu).  
- Ratkaisu: **pienenn√§ oppimisnopeutta v√§hitellen** varmistaaksesi konvergenssin.  

---

## Osa 4 : Yhden neuronin rajat ‚Äî XOR ja toinen talvi

![Board notes](/images/docs/mlLecture4_Neuralnetworks/5.png)

### 4.1 Lineaarinen erotettavuus
- Perceptron voi ratkaista vain ongelmat, joissa data on **lineaarisesti erotettavissa** (yksi viiva/hyper-taso jakaa luokat).  
- Esimerkki: AND, OR ovat ratkaistavissa.  

### 4.2 XOR-ongelma
- XOR-totuustaulu:  
  - (0,0) ‚Üí 0  
  - (1,0) ‚Üí 1  
  - (0,1) ‚Üí 1  
  - (1,1) ‚Üí 0  
- Ei lineaarisesti erotettavissa ‚Äî yksitt√§inen perceptron ei voi ratkaista t√§t√§.  

### 4.3 Toinen AI-talvi
- Vuonna 1969 Minsky & Papert osoittivat perceptronien rajoitukset (erityisesti XOR).  
- Johti skeptisyyteen ja rahoituksen v√§henemiseen ‚Üí ‚Äútoinen talvi.‚Äù  

### 4.4 Monikerrosidea
- Biologisilla aivoilla on **monia neuroneja, jotka toimivat yhdess√§**.  
- Jos yhdist√§mme useita neuroneja (piilokerros), XOR voidaan esitt√§√§.  
- Takaisinlevitysalgoritmi (1970‚Äì80-luvuilla) mahdollisti t√§llaisten verkkojen tehokkaan koulutuksen.  

---
