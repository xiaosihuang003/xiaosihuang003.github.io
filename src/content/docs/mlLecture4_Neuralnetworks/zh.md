---
title: "ML_4: 神经网络"
subtitle: "DATA.ML.100 · Joni Kämäräinen · 2025年9月4日 周四 TB104"
date: "2025-09-08"
lang: "zh"
excerpt: "神经网络的基础：从生物启发到LTU，Hebbian学习，XOR限制，非线性激活，反向传播，以及现代框架（TensorFlow与PyTorch）。"
tags: ["Joni Kämäräinen", "机器学习", "神经网络", "TensorFlow", "PyTorch"]
draft: false
---

## 第1部分：从大脑到神经网络

![板书](/images/docs/mlLecture4_Neuralnetworks/1.png)

### 1.1 为什么研究神经网络？
- 工程师在遇到瓶颈时，常常会借鉴其他科学（生物学、人类科学等）的思想。  
- 人脑是目前已知最智能的“计算机”，因此成为机器学习的重要灵感来源。  

### 1.2 大脑与进化
- 脑容量受到 **头骨/产道** 的限制，不可能无限增大。  
- 食物资源也会影响进化（例如岛屿上的种群往往变小）。  
- 不同物种的神经元数量：  
  - 海星：~500  
  - 水蛭：~10,000  
  - 果蝇：~100,000  
  - 蟑螂：~1,000,000  
  - 乌龟：~10,000,000  
  - 仓鼠：~100,000,000  
  - 猫头鹰：~1,000,000,000  
  - 大猩猩：~30,000,000,000  
  - 人类：~100,000,000,000  
- 关键不仅在于神经元的数量，而在于 **连接数**（人脑约有 ~$10^{13}$ 个突触）。  

### 1.3 生物神经元结构
- **树突** = 输入 ($x_1, x_2, …$)  
- **突触** = 权重（可能兴奋或抑制）  
- **细胞体 (soma)** = 计算单元（类似线性模型 + 激活函数）  
- **轴突** = 输出 ($y$)  

➡️ **人工神经元模型 = 输入 × 权重 → 加和 → 激活函数 → 输出**

### 1.4 注释
- 生物神经元通过 **脉冲** 传递信号，而人工神经元使用连续值。  
- 由此产生了一些新兴研究方向：  
  - **脉冲神经网络 (SNNs)**  
  - **类脑芯片 (Neuromorphic chips)**

### 1.5 人工神经元作为线性模型
- 从本质上讲，人工神经元就是一个 **线性模型**：输入的加权和加上偏置。  
- 这与我们之前学习的 **线性回归** 和 **分类** 完全一致。  
- 这些早期主题为理解神经网络奠定了基础。  

---

## 第2部分：线性模型与第一次神经网络浪潮 (1943)

![板书](/images/docs/mlLecture4_Neuralnetworks/2.png)

### 2.1 神经元作为线性模型
- 神经元可以建模为 **输入的线性组合** 加上偏置：  
  $$
  y = \sum_i w_i x_i + w_0
  $$
- 该系统已经可以执行 **线性回归**。  
- 但用于分类时，线性回归并不足够 → 需要阈值函数或非线性激活。  

### 2.2 向分类迈进
- 二分类：只有两类 (0/1)。  
- 解决方法：使用 **阈值函数**（后来发展为 sigmoid/logistic）。  
- 代码思路：  
  - 先计算加权和。  
  - 然后用阈值（或 sigmoid）决定输出。  

### 2.3 历史注解 — 第一次浪潮 (1943)
- **McCulloch & Pitts (1943): 线性阈值单元 (LTU)**  
- 定义：  
  $$
  y = 
    \begin{cases}
      1 & \text{如果 } w_1x_1 + w_2x_2 + w_0 \geq \theta \\
      0 & \text{如果 } w_1x_1 + w_2x_2 + w_0 < \theta
    \end{cases}
  $$
- 一般取阈值 $\theta = 0$。  
- 目标：模拟逻辑门（数字计算机的基础）。  

### 2.4 示例：与门 (AND gate)
- 输入: $x_1, x_2 \in \{0,1\}$  
- 期望输出: 仅当两个输入均为1时 $y=1$。  
- 选择权重: $w_1 = w_2 = \tfrac{2}{3},\ w_0 = -1$  

验证：  
$$
\begin{aligned}
(0,0):\ & -1<0 \ \Rightarrow y=0 \\
(1,0)\ \text{或}\ (0,1):\ & \tfrac{2}{3}-1=-\tfrac{1}{3}<0 \ \Rightarrow y=0 \\
(1,1):\ & \tfrac{4}{3}-1=\tfrac{1}{3}>0 \ \Rightarrow y=1
\end{aligned}
$$  

✅ 正确实现了与门。  

### 2.5 示例：或门 (OR gate)
![板书](/images/docs/mlLecture4_Neuralnetworks/3.png)

- 输入: $x_1, x_2 \in \{0,1\}$  
- 期望输出: 如果任一输入为1，则 $y=1$。  
- 选择权重 (基础版): $w_1 = w_2 = 1,\ w_0 = -1$  
  - (0,0): $-1<0 \Rightarrow y=0$  
  - (1,0) 或 (0,1): $1-1=0 \Rightarrow y=1$  
  - (1,1): $2-1=1>0 \Rightarrow y=1$  

✅ 可以实现或门，但边界敏感。  

改进版: $w_1 = w_2 = \tfrac{3}{2},\ w_0 = -1$ → 更稳定。  

### 2.6 作业：NAND 门
> 请用 LTU 模型设计一个 **NAND 门**。  
> - 提示: NAND = NOT(AND)。  
> - 从 AND 门的权重开始调整。  

### 2.7 关键总结
- 简单的 LTU 通过合适的权重就能实现逻辑函数。  
- 这是历史上首次证明神经元在原理上可以作为计算的基础。  

---
## 第3部分：第二次浪潮 (1949) — Hebbian 学习

![板书](/images/docs/mlLecture4_Neuralnetworks/4.png)

### 3.1 Hebbian 学习规则
- 被提出作为 LTU 权重更新机制。  
- 更新公式：  
  $$
  w_i^{t+1} = w_i^t + \eta (y - \hat{y}) x
  $$
- 其中：  
  - $y$ = 真实输出（标签）  
  - $\hat{y}$ = 预测输出  
  - 误差 = $y - \hat{y}$  
  - $\eta$ = 学习率（如 0.1）  

### 3.2 学习动态
- 如果预测值过高 → 权重减小。  
- 如果预测值过低 → 权重增大。  
- 随着迭代，权重逐渐收敛，使 $y \approx \hat{y}$。  

### 3.3 学习率调度
- 过大的 $\eta$ 会导致震荡（误差不收敛）。  
- 解决办法：**逐步减小学习率** 以保证收敛。  

---

## 第4部分：单个神经元的局限 — XOR 与第二次冬天

![板书](/images/docs/mlLecture4_Neuralnetworks/5.png)

### 4.1 线性可分性
- 感知机只能解决 **线性可分问题**（一条直线/超平面就能分开类别）。  
- 示例：AND、OR 可以解决。  

### 4.2 XOR 问题
- XOR 真值表：  
  - (0,0) → 0  
  - (1,0) → 1  
  - (0,1) → 1  
  - (1,1) → 0  
- 该问题 **不可线性可分** → 单个感知机无法解决。  

### 4.3 第二次 AI 冬天
- 1969 年：Minsky & Papert 指出感知机的局限（尤其是 XOR）。  
- 导致学界质疑，资金减少 → **第二次冬天**。  

### 4.4 多层思路
- 生物大脑依靠 **许多神经元协同工作**。  
- 如果组合多个神经元（隐层），XOR 是可以表示的。  
- 1970–80年代的 **反向传播算法** 使得训练这类网络成为可能。  

---

## 第5部分：从线性到非线性 — Logistic、损失函数与梯度下降

![板书](/images/docs/mlLecture4_Neuralnetworks/6.png)

### 5.1 激活函数
- 为了超越线性回归，需要使用非线性激活：  
  - **Sigmoid (logistic)：**  
    $$\sigma(z) = \frac{1}{1+e^{-z}} \quad \in (0,1)$$  
  - **Tanh：**  
    $$\tanh(z) \in (-1,1)$$  
  - 其他：线性、ReLU 等（后续发展）。  

### 5.2 损失函数
- 回归：均方误差 (MSE)  
  $$L = \frac{1}{N} \sum (y - \hat{y})^2$$  
- 分类：交叉熵（后来引入）。  

### 5.3 梯度下降
- 计算损失函数对权重的梯度：  
  $$\nabla_w L = \left( \frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_2}, … \right)$$  
- 更新公式：  
  $$
  w^{(t+1)} = w^{(t)} - \eta \, \nabla_w L
  $$
- $\eta$ = 学习率 — 太大 → 发散；太小 → 收敛慢。  
- **学习率调度** 有助于收敛。  

### 5.4 万能近似定理
- 至少有 **一个隐层** 且使用非线性激活时，神经网络可以近似 **任意函数**（Cybenko 1989）。  

### 5.5 多层感知机与反向传播
![板书](/images/docs/mlLecture4_Neuralnetworks/7.png)

- 单个 LTU 无法解决 XOR 等问题。  
- 解决办法：将多个神经元组合成 **多层感知机 (MLP)**。  
  - 示例：两个隐层神经元 ($y_1, y_2$) 连接到输出神经元。  
  - 这样可以产生非线性决策边界。  

- **损失函数**：仍使用均方误差 (MSE)。  
- **训练难点**：如何更新隐层的权重？  

➡️ **反向传播算法**  
- 计算输出误差（预测与目标的差异）。  
- 用 **链式法则** 将误差向后传播。  
- 更新所有权重（输入→隐层，隐层→输出）。  

👉 这一突破使得深度网络的训练成为可能，克服了 XOR 的限制。  

### 5.6 多分类问题
![板书](/images/docs/mlLecture4_Neuralnetworks/8.png)

- 之前我们只讨论了 **二分类** (0/1)。  
- 对于 **多分类 (N 类)**：  
  - 输出层有 **N 个神经元**，每个对应一个类别。  
  - 示例：3 类 → 输出 = $(y_1, y_2, y_3)$。  
- **标签** 使用 **独热编码 (one-hot encoding)**：  
  - 类别1 → (1,0,0)  
  - 类别2 → (0,1,0)  
  - 类别3 → (0,0,1)  
- 训练过程：  
  - 前向传播：计算所有类别的输出。  
  - 损失函数：常用 **交叉熵 + softmax**。  
  - 反向传播：更新所有连接的权重。  

➡️ 这样神经网络就能处理 **任意类别数** 的问题。  

---

## 第6部分：实践 — TensorFlow 与 PyTorch

![板书](/images/docs/mlLecture4_Neuralnetworks/9.png)

### 6.1 TensorFlow 与 PyTorch
- 两者都是 **主流深度学习框架**。  

- **TensorFlow (Google)**  
  - 本课程选择它 *“为了多样性”*。  
  - `Sequential` API 非常简单：可以逐层添加网络。  
  - 适合做 **入门演示**（如线性回归）。  
  - 安装时可能会报 *“一些小错误”*，但基本能用。  

- **PyTorch (Meta/Facebook)**  
  - *“我认为 PyTorch 已经比 TensorFlow 更受欢迎。”*  
  - 更灵活，更接近原生 Python，适合复杂模型。  
  - *“PyTorch 更有教育意义，但也更难。”*  
  - 在 **深度学习课程** 中会使用。  

- **总结**  
  - **TensorFlow** → 简单、适合初学者、课程演示。  
  - **PyTorch** → 研究更流行、灵活，但更难。  

- 讲课原话：  
  > *“没有人会手动做反向传播，除非是学习时练习。实际工作中我们用 TensorFlow 或 PyTorch。”*  

---

### 6.2 单神经元回归
- 模型：`Dense(units=1, activation="linear")`  
- 优化器：SGD（随机梯度下降）。  
- 损失：MSE。  
- 训练 (`fit`) 显示误差随 epoch 减小。  
- ✅ 学会了拟合一条直线（类似线性回归）。  

---

### 6.3 多层感知机 (MLP) 回归
- 任务：近似一个 **正弦波**。  
- 输入：时间 $t$。  
- 隐层：50 个神经元，**sigmoid 激活**。  
- 输出：$y \approx \sin(t)$。  
- 展示了 **万能近似定理**：  
  - 只要神经元足够多，一个隐层就能近似任意函数。  
- 训练时需要调整 **学习率** 和 **epoch** 来减少震荡与误差。  

---

### 6.4 分类示例
- 任务：区分 “**霍比特人 vs. 精灵**”。  
- 输出层：2 个神经元（二分类）。  
- 标签采用 **独热编码**：  
  - 霍比特人 → (1,0)  
  - 精灵 → (0,1)  
- 损失函数：交叉熵。  
- 准确率随 epoch 提升（如从 86% → 92%）。  

---

### 6.5 作业
- 用 **TensorFlow** 训练你自己的神经网络。  
- 可以尝试：  
  - 不同的学习率。  
  - 更多的训练轮数。  
  - 更多的隐层神经元/层数。  
- 挑战：超越 **Hebbian 学习** 和课堂演示的准确率。  

---

## 总结

1. **从大脑到神经网络**  
   - 神经网络受生物神经元启发：输入、权重、加和、激活。  
   - 大脑展示了庞大连接的威力 (~$10^{13}$ 突触)。  

2. **第一次浪潮 (1943)：LTU**  
   - McCulloch & Pitts 证明单个神经元能实现逻辑函数（AND、OR、NAND）。  
   - 但当时没有学习机制。  

3. **第二次浪潮 (1949)：Hebbian 学习**  
   - 提出了基于误差调整权重的思想。  
   - 是早期学习方法，但对学习率和收敛敏感。  

4. **单神经元的局限 (XOR)**  
   - 感知机只能解决线性可分问题。  
   - XOR 揭示了局限 → 导致怀疑和第二次 AI 冬天。  

5. **非线性激活与反向传播**  
   - logistic、tanh 等激活函数使网络能表示非线性边界。  
   - 反向传播 (1970–80年代) 使训练多层感知机成为可能。  
   - 万能近似定理 (1989)：一个隐层足以近似任意函数。  

6. **现代实践：TensorFlow 与 PyTorch**  
   - TensorFlow：简单、适合初学者、用于课堂。  
   - PyTorch：更灵活，研究更流行，在深度学习课程中使用。  
   - 框架让回归、分类、多层网络的训练变得切实可行。  
