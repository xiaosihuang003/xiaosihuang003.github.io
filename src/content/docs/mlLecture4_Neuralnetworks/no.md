---
title: "Maskinl√¶ring 4: Nevrale nettverk"
subtitle: "DATA.ML.100 ¬∑ Joni K√§m√§r√§inen ¬∑ Tor 4.9.2025 TB104 üòä"
date: "2025-09-08"
lang: "no"
excerpt: "Grunnlaget for nevrale nettverk: fra biologisk inspirasjon til LTU-er, Hebbiansk l√¶ring, XOR-begrensninger, ikke-line√¶re aktiveringer, backpropagation og moderne rammeverk (TensorFlow & PyTorch)."
tags: ["Joni K√§m√§r√§inen", "machine-learning", "neural-networks", "TensorFlow", "PyTorch"]
draft: false
---

## Del 1 : Fra hjerner til nevrale nettverk

![Board notes](/images/docs/mlLecture4_Neuralnetworks/1.png)

### 1.1 Hvorfor studere nevrale nettverk?
- N√•r ingeni√∏rer m√∏ter flaskehalser, l√•ner de ofte ideer fra andre fag (biologi, humaniora, osv.).  
- Den menneskelige hjernen er den mest intelligente ‚Äúdatamaskinen‚Äù vi kjenner, og inspirerer maskinl√¶ring.  

### 1.2 Hjerne og evolusjon
- Hjernest√∏rrelse begrenses av **skalle/f√∏dselskanal** ‚Äî den kan ikke vokse uendelig.  
- Mattilgang p√•virker ogs√• evolusjon (f.eks. √∏ybefolkninger som blir mindre).  
- Antall nevroner hos ulike arter:
  - Sj√∏stjerne: ~500  
  - Iglen: ~10 000  
  - Bananflue: ~100 000  
  - Kakerlakk: ~1 000 000  
  - Skilpadde: ~10 000 000  
  - Hamster: ~100 000 000  
  - Ugle: ~1 000 000 000  
  - Gorilla: ~30 000 000 000  
  - Menneske: ~100 000 000 000  
- N√∏kkelen er ikke bare antall nevroner, men **antall forbindelser** (~$10^{13}$ synapser i menneskehjernen).  

### 1.3 Biologisk nevronstruktur
- **Dendritter** = inndata ($x_1, x_2, ‚Ä¶$)  
- **Synapser** = vekter (kan eksitere eller hemme)  
- **Soma (cellekropp)** = beregningsenhet (som line√¶r modell + aktivering)  
- **Akson** = utdata ($y$)  

‚û°Ô∏è **Kunstig nevron = Inndata √ó Vekter ‚Üí Summering ‚Üí Aktiveringsfunksjon ‚Üí Utdata**

### 1.4 Notater
- Biologiske nevroner sender **spiker** (spikes), mens kunstige bruker kontinuerlige verdier.  
- Dette gir felt som:  
  - **Spiking Neural Networks (SNNs)**  
  - **Nevromorfe brikker**

### 1.5 Kunstig nevron som line√¶r modell
- Et kunstig nevron kan beskrives som en **line√¶r modell**: vektet sum av inndata + bias.  
- Dette kobler direkte til **line√¶r regresjon** og **klassifisering**.  
- Disse emnene er grunnmuren for √• forst√• nevrale nettverk.

---

## Del 2 : Line√¶r modell og den f√∏rste b√∏lgen (1943)

![Board notes](/images/docs/mlLecture4_Neuralnetworks/2.png)

### 2.1 Line√¶r modell som nevron
- Nevronet kan modelleres som en **line√¶r kombinasjon** av inndata + bias:  
  $$
  y = \sum_i w_i x_i + w_0
  $$
- Dette kan allerede gj√∏re **line√¶r regresjon**.  
- For klassifisering er ikke ren regresjon nok ‚Üí vi trenger terskel eller ikke-line√¶r aktivering.  

### 2.2 Mot klassifisering
- Bin√¶r klassifisering: to klasser (0/1).  
- L√∏sning: bruk en **terskelfunksjon** (senere sigmoid/logistisk).  
- I kode:  
  - F√∏rst vektet sum.  
  - S√• terskel (eller sigmoid) for √• avgj√∏re utdata.  

### 2.3 Historisk ‚Äî B√∏lge 1 (1943)
- **McCulloch & Pitts (1943): Linear Threshold Unit (LTU)**  
- Definisjon:  
  $$
  y = 
    \begin{cases}
      1 & \text{hvis } w_1x_1 + w_2x_2 + w_0 \geq \theta \\
      0 & \text{hvis } w_1x_1 + w_2x_2 + w_0 < \theta
    \end{cases}
  $$
- Ofte velger man $\theta = 0$.  
- Motivasjon: simulere logiske porter (grunnlaget for digitale datamaskiner).  

### 2.4 Eksempel: AND-port
- Inndata: $x_1, x_2 \in \{0,1\}$  
- √ònsket utdata: $y=1$ bare n√•r begge er 1.  
- Velg vekter: $w_1 = w_2 = \tfrac{2}{3},\ w_0 = -1$  

Sjekk:  
$$
\begin{aligned}
(0,0):\ & -1<0 \ \Rightarrow y=0 \\
(1,0)\ \text{eller}\ (0,1):\ & \tfrac{2}{3}-1=-\tfrac{1}{3}<0 \ \Rightarrow y=0 \\
(1,1):\ & \tfrac{4}{3}-1=\tfrac{1}{3}>0 \ \Rightarrow y=1
\end{aligned}
$$  

‚úÖ Fungerer som AND.  

### 2.5 Eksempel: OR-port
![Board notes](/images/docs/mlLecture4_Neuralnetworks/3.png)

- Inndata: $x_1, x_2 \in \{0,1\}$  
- √ònsket: $y=1$ hvis minst √©n er 1.  
- Enkelt valg: $w_1 = w_2 = 1,\ w_0 = -1$  
  - (0,0): $-1<0 \Rightarrow y=0$  
  - (1,0) eller (0,1): $1-1=0 \Rightarrow y=1$  
  - (1,1): $2-1=1>0 \Rightarrow y=1$  

‚úÖ Fungerer, men marginen er tynn.  

Bedre margin: $w_1 = w_2 = \tfrac{3}{2},\ w_0 = -1$.  

### 2.6 Hjemmeoppgave: NAND
> Design en **NAND-port** med LTU.  
> - Hint: NAND = NOT(AND).  
> - Start fra AND-vektene og just√©r.  

### 2.7 Takeaway
- En enkel LTU med riktige vekter kan implementere logikk.  
- Dette viste at nevroner prinsipielt kan utgj√∏re grunnlag for beregning.

---

## Del 3 : B√∏lge 2 (1949) ‚Äî Hebbiansk l√¶ring

![Board notes](/images/docs/mlLecture4_Neuralnetworks/4.png)

### 3.1 Hebbiansk l√¶ringsregel
- Foresl√•tt som oppdateringsmekanisme for LTU-vekter.  
- Regel:  
  $$
  w_i^{t+1} = w_i^t + \eta (y - \hat{y}) x
  $$
- Der:  
  - $y$ = sann utdata (fasit)  
  - $\hat{y}$ = estimert utdata  
  - feil = $y - \hat{y}$  
  - $\eta$ = l√¶ringsrate (f.eks. 0,1)  

### 3.2 L√¶ringsdynamikk
- Estimat for h√∏yt ‚Üí vekter ned.  
- Estimat for lavt ‚Üí vekter opp.  
- Over tid n√¶rmer $y \approx \hat{y}$.  

### 3.3 Plan for l√¶ringsrate
- Stor $\eta$ kan gi oscillasjoner.  
- **Reduser gradvis** for bedre konvergens.  

---

## Del 4 : Begrensninger for ett nevron ‚Äî XOR og den andre vinteren

![Board notes](/images/docs/mlLecture4_Neuralnetworks/5.png)

### 4.1 Line√¶r separabilitet
- En perceptron l√∏ser bare **line√¶rt separerbare** problemer.  
- AND, OR fungerer.  

### 4.2 XOR-problemet
- XOR:  
  - (0,0) ‚Üí 0  
  - (1,0) ‚Üí 1  
  - (0,1) ‚Üí 1  
  - (1,1) ‚Üí 0  
- Ikke line√¶rt separerbar ‚Üí kan ikke l√∏ses av √©n perceptron.  

### 4.3 Den andre AI-vinteren
- 1969: Minsky & Papert viste perceptron-begrensninger (s√¶rlig XOR).  
- Skapte skepsis og kutt i finansiering ‚Üí ‚Äúandre vinteren‚Äù.  

### 4.4 Id√©en om flere lag
- Hjerner har **mange nevroner som jobber sammen**.  
- Med skjult lag kan XOR representeres.  
- **Backpropagation** (1970‚Äì80-tallet) gjorde treningen effektiv.  

---

## Del 5 : Fra line√¶r til ikke-line√¶r ‚Äî Logistikk, tap & gradientnedstigning

![Board notes](/images/docs/mlLecture4_Neuralnetworks/6.png)

### 5.1 Aktiveringsfunksjoner
- For √• komme forbi line√¶r regresjon bruker vi ikke-line√¶re aktiveringer:  
  - **Sigmoid (logistisk):**  
    $$\sigma(z) = \frac{1}{1+e^{-z}} \quad \in (0,1)$$  
  - **Tanh:**  
    $$\tanh(z) \in (-1,1)$$  
  - Line√¶r (identitet), ReLU, osv.  

### 5.2 Tapsfunksjoner
- Regresjon: MSE  
  $$L = \frac{1}{N} \sum (y - \hat{y})^2$$  
- Klassifisering: Kryssentropi (oftest sammen med softmax).  

### 5.3 Gradientnedstigning
- Gradient av tapet mht. vekter:  
  $$\nabla_w L = \left( \frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_2}, ‚Ä¶ \right)$$  
- Oppdatering:  
  $$
  w^{(t+1)} = w^{(t)} - \eta \, \nabla_w L
  $$
- $\eta$ for stor ‚Üí divergens, for liten ‚Üí tregt.  
- **Planlagt l√¶ringsrate** hjelper.  

### 5.4 Universell approksimasjon
- Med minst **ett skjult lag** og ikke-line√¶r aktivering kan et nettverk approksimere *enhver* funksjon (Cybenko 1989).  

### 5.5 Flerlagede perceptron & backpropagation
![Board notes](/images/docs/mlLecture4_Neuralnetworks/7.png)

- √ân LTU klarer ikke XOR.  
- L√∏sning: **MLP** (flere nevroner/lag).  
  - Eks.: to skjulte nevroner ($y_1, y_2$) inn i √©n utgangsnevron.  
  - Gir ikke-line√¶re beslutningsgrenser.  

- **Tap**: fortsatt MSE i eksempelet.  
- **Utfordring**: hvordan trene skjulte lag?  

‚û°Ô∏è **Backpropagation**  
- Regn ut utgangsfeil (prediksjon ‚àí m√•l).  
- Propager bakover med **kjerneregel** (chain rule).  
- Oppdater alle vekter via gradientnedstigning.  

Gjennombruddet p√• 1970‚Äì80-tallet gjorde dype nett praktiske og overvant XOR-floken.

### 5.6 Flerklasse-klassifisering

![Board notes](/images/docs/mlLecture4_Neuralnetworks/8.png)

- Til n√•: **bin√¶rt** (0/1).  
- For **N klasser**:  
  - Utdatalaget har **N nevroner**, ett per klasse.  
  - Eksempel: 3 klasser ‚Üí utdata $(y_1, y_2, y_3)$.  
- **One-hot-koding** av fasit:  
  - Klasse 1 ‚Üí (1,0,0), Klasse 2 ‚Üí (0,1,0), ‚Ä¶  
- Trening:  
  - Framoverpass ‚Üí utdata.  
  - Tap: ofte **kryssentropi** + softmax.  
  - Bakoverpass: backprop gjennom alle vekter.  

‚û°Ô∏è Nett kan h√•ndtere **vilk√•rlig antall** klasser.

---

## Del 6 : Praktisk arbeid ‚Äî TensorFlow & PyTorch

![Board notes](/images/docs/mlLecture4_Neuralnetworks/9.png)

### 6.1 TensorFlow vs. PyTorch
- Begge er **mainstream dyp-l√¶ringsrammeverk**.  

- **TensorFlow (Google)**  
  - Valgt her *‚Äúfor mangfoldets skyld.‚Äù*  
  - `Sequential`-API er veldig enkelt (legg til lag etter lag).  
  - Flott for **f√∏rste demo** (f.eks. line√¶r regresjon).  
  - Installasjon kan gi *‚Äúnoen sm√• feil‚Äù* men funker.  

- **PyTorch (Meta/Facebook)**  
  - *‚ÄúPyTorch er nok mer popul√¶rt enn TensorFlow.‚Äù*  
  - Mer fleksibelt og Python-n√¶rt; bra for komplekse modeller.  
  - *‚ÄúPyTorch er mer l√¶rerikt, men ogs√• vanskeligere.‚Äù*  
  - Brukes i **Deep Learning-kurs** senere.  

- **Oppsummert**  
  - **TensorFlow** ‚Üí enklere, nybegynnervennlig, bra for demo.  
  - **PyTorch** ‚Üí popul√¶rt i forskning, fleksibelt, mer l√¶rerikt men vanskeligere.  

- Sitat:  
  > *‚ÄúIngen gj√∏r backprop manuelt, bortsett fra for √• l√¶re. Til faktisk arbeid bruker vi TensorFlow eller PyTorch.‚Äù*  

---

### 6.2 Enkel regresjon med ett nevron
- Modell: `Dense(units=1, activation="linear")`  
- Optimalisering: SGD.  
- Tap: MSE.  
- Trening (`fit`) viser fallende feil pr. epoke.  
- ‚úÖ L√¶rer en rett linje (som line√¶r regresjon).  

---

### 6.3 MLP-regresjon (sinus)
- Oppgave: approksimer **sinus**.  
- Inndata: $t$ (tid).  
- Skjult lag: 50 nevroner, **sigmoid**.  
- Utdata: $y \approx \sin(t)$.  
- Viser **universell approksimasjon**.  
- Just√©r **l√¶ringsrate** og **antall epoker** for stabilitet.  

---

### 6.4 Klassifiseringseksempel
- Oppgave: ‚Äú**Hobbits vs. Alver**‚Äù.  
- Utgang: 2 nevroner (bin√¶rt).  
- **One-hot** etiketter: Hobbit ‚Üí (1,0), Alv ‚Üí (0,1).  
- Tap: kryssentropi.  
- Presisjon √∏ker med epoker (f.eks. 86% ‚Üí 92%).  

---

### 6.5 Hjemmeoppgave
- Tren egne nett i **TensorFlow**.  
- Pr√∏v:  
  - Ulike l√¶ringsrater.  
  - Flere epoker.  
  - Flere skjulte nevroner/lag.  
- Utfordring: sl√• **Hebbiansk l√¶ring** og demoene.  

---

## Oppsummering

1. **Fra hjerne til nett**  
   - Nevrale nettverk forenkler nevroner: inndata, vekter, sum, aktivering.  
   - Hjernen viser kraften i ~$10^{13}$ synapser.  

2. **B√∏lge 1 (1943): LTU**  
   - Logiske porter (AND/OR/NAND) kan lages av √©n enhet.  
   - Ingen l√¶ring enn√•.  

3. **B√∏lge 2 (1949): Hebbiansk l√¶ring**  
   - Just√©r vekter ut fra feil; f√∏lsomt for l√¶ringsrate.  

4. **XOR-begrensningen**  
   - √ân perceptron l√∏ser bare line√¶rt separerbare problemer ‚Üí ‚Äúandre AI-vinter‚Äù.  

5. **Ikke-linearitet & backprop**  
   - Sigmoid/tanh gir ikke-line√¶re grenser.  
   - Backprop (70‚Äì80-tallet) muliggjorde MLP-trening.  
   - 1989: Ett skjult lag kan approksimere enhver funksjon.  

6. **Praksis: TensorFlow & PyTorch**  
   - TF: nybegynnervennlig demo-valg.  
   - PyTorch: fleksibelt, forskningstungt.  
   - Rammeverk gj√∏r skalerbar trening praktisk.
